{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJh1ku7qGt/myIM2hEirwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c27c8b899d30496bad4507eaace9c8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0093ad54b8764c34b1ed99038faca397",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_80e20190685d4995bd21f702342d4a60",
              "IPY_MODEL_9cffbb15f9fe40448ac98f2d1ac4e156"
            ]
          }
        },
        "0093ad54b8764c34b1ed99038faca397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80e20190685d4995bd21f702342d4a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef178fdb09384eb29384bac312f711af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59eff89a36cc4d09a27d0337a13ff03b"
          }
        },
        "9cffbb15f9fe40448ac98f2d1ac4e156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_542bec5bc7a84160ab97d5e80e5f5d4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26427392/? [00:20&lt;00:00, 14724221.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fa0dd7659874ff084b7c6d82215ea19"
          }
        },
        "ef178fdb09384eb29384bac312f711af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59eff89a36cc4d09a27d0337a13ff03b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "542bec5bc7a84160ab97d5e80e5f5d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fa0dd7659874ff084b7c6d82215ea19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "559b9581d2984f4fbbc18088dfc2283d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3869915b035445e2921db7489c60f8c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cacd01381afd4ffe88e17862faf941f5",
              "IPY_MODEL_5e042e95ad994246bbfcb6dd6376ff9b"
            ]
          }
        },
        "3869915b035445e2921db7489c60f8c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cacd01381afd4ffe88e17862faf941f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2793360258b14103af2c586e975c0762",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_791d64d351064b66a3274fe70ea06529"
          }
        },
        "5e042e95ad994246bbfcb6dd6376ff9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c5f759678aa47ee95e5d6da62b0e9b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/29515 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e85f08819a44aa0a703483e385e641b"
          }
        },
        "2793360258b14103af2c586e975c0762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "791d64d351064b66a3274fe70ea06529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c5f759678aa47ee95e5d6da62b0e9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e85f08819a44aa0a703483e385e641b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4f22f9810d9446690ffcaf38e5aa149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2159b20a463346d3bb475924ee15fbe6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e13970df19c2485693127b4a65d2f4cb",
              "IPY_MODEL_9466f30a7d22450281c2035973e36de0"
            ]
          }
        },
        "2159b20a463346d3bb475924ee15fbe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e13970df19c2485693127b4a65d2f4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3c5a5066e0f41529dff66f964175431",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d06978b6fece40c1b226941c1e38efcd"
          }
        },
        "9466f30a7d22450281c2035973e36de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_50f3a1d940894b05a9bc1f577f1d4536",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4423680/? [00:17&lt;00:00, 1496300.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15803bacc2974c9a8d211dc70d87e1c3"
          }
        },
        "e3c5a5066e0f41529dff66f964175431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d06978b6fece40c1b226941c1e38efcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50f3a1d940894b05a9bc1f577f1d4536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15803bacc2974c9a8d211dc70d87e1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee42f928ca0b4aa9af9bddacf29aa813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe9588d7a0414c259e4f0a25ef58b7e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13264f0318da4c3ebfb4f490f773b883",
              "IPY_MODEL_637a25c191b540dfaac9e631b377157f"
            ]
          }
        },
        "fe9588d7a0414c259e4f0a25ef58b7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13264f0318da4c3ebfb4f490f773b883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_47f7ebfda11940d19c1977bad007888c",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22d915b925d64faa9305ea7d0ac1d74a"
          }
        },
        "637a25c191b540dfaac9e631b377157f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e08372168f5497dbf3965bff3b70372",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5148 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a451682dfbf4b86a6db026931bdd02f"
          }
        },
        "47f7ebfda11940d19c1977bad007888c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22d915b925d64faa9305ea7d0ac1d74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e08372168f5497dbf3965bff3b70372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a451682dfbf4b86a6db026931bdd02f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokulanv/ToyFederatedLearning/blob/master/FederatedLearning/RemoteFL_MNISTData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odP0S-VeinQd",
        "colab_type": "text"
      },
      "source": [
        "# F Learning\n",
        "\n",
        "### Idea:\n",
        "Instead of having the data locally and training the model based on the local data, Federated learning pushes the model to where the data resides and trains the model remotely and sends the updates back to the centralized server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjMInwo_iBGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72481536-6fd7-49e0-d2a2-896589760d94"
      },
      "source": [
        "pip install syft"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "  Using cached https://files.pythonhosted.org/packages/43/29/3fd78b6cecc540ebb31e676d269787d4bf5b4b90e479474a4f5ec9744bc0/syft-0.2.6-py3-none-any.whl\n",
            "Collecting torch~=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 18kB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (from syft) (4.5.3)\n",
            "Requirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.0)\n",
            "Collecting psutil==5.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.18.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.18.4)\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting Pillow~=6.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/bbbc569f98f47813c50a116b539d97b3b17a86ac7a309f83b2022d26caf2/Pillow-6.2.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1)\n",
            "Collecting websockets~=8.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting lz4~=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/81/011fef8766fb0ef681037ad6fee96168ee03a864464986cbaa23e5357704/lz4-3.0.2-cp36-cp36m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 47.6MB/s \n",
            "\u001b[?25hCollecting torchvision~=0.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting websocket-client~=0.57.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib~=1.6.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.6.0)\n",
            "Collecting phe~=1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n",
            "Collecting aiortc==0.9.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/54/beb8dcb9050aaf903c4426884970d0f18a76e49c69b5a06653812e404446/aiortc-0.9.28-cp36-cp36m-manylinux2010_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 45.7MB/s \n",
            "\u001b[?25hCollecting syft-proto~=0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/38/12c59bf372b87044c3338439a6194540d00a043009d2fdf0fc24998ba34d/syft_proto-0.4.5-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hCollecting requests~=2.22.0\n",
            "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.2)\n",
            "Collecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/97/00741edd49788510b834b60a1a4d0afb2c4942770c11b8e0f6e914371718/python_socketio-4.6.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision~=0.5.0->syft) (1.12.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (1.14.0)\n",
            "Collecting pylibsrtp>=0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/cc/89985f4c15320ed0d52b4074fd6f7baa6dd567d79414c10c9dbd9ade39b2/pylibsrtp-0.6.6-cp36-cp36m-manylinux2010_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (0.7)\n",
            "Collecting av<9.0.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/94/7a821dae6f70e30b1d2bfbdf734494f957fda907e453b27a551bf2949fe4/av-8.0.1-cp36-cp36m-manylinux2010_x86_64.whl (30.0MB)\n",
            "\u001b[K     |████████████████████████████████| 30.0MB 147kB/s \n",
            "\u001b[?25hCollecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/86/e3cdf660b67da7a9a7013253db5db7cf786a52296cb40078db1206177698/aioice-0.6.18-py3-none-any.whl\n",
            "Collecting pyee>=6.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/28/1cedd44c27907f1507a28ff2d36fc6cdb981c9deff2fa288bc48a700c7c9/pyee-7.0.2-py2.py3-none-any.whl\n",
            "Collecting crc32c\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/82/f60248c01a8a23ae07bd4c43d78d69b20ffe324311db3b0785e391aa09d2/crc32c-2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting protobuf>=3.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/c1/300e675e55644f400b9e855c6a8189f3c9e4f41906a23954d15ebf59600f/protobuf-3.12.1-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 38.6MB/s \n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (1.24.3)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.1.0)\n",
            "Collecting python-engineio>=3.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/7e/d14f8ea2ad1aa7d28f8d95ffab8ec7a6034d81f7f52ef4d788c8e89cdcdc/python_engineio-3.13.0-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->aiortc==0.9.28->syft) (2.20)\n",
            "Collecting netifaces\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.1->syft-proto~=0.4.5->syft) (46.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask~=1.1.1->syft) (1.1.1)\n",
            "Building wheels for collected packages: psutil, phe\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=272669 sha256=2bfbea485363e3cc694e2782935ddf9c36c373d4525a6353fcaf9b33383d033a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=3fef4d39bc9613f535a83d8b91332862f475371af1cda8f63eb71f6430fcca96\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n",
            "Successfully built psutil phe\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, psutil, python-engineio, python-socketio, flask-socketio, Pillow, websockets, lz4, torchvision, websocket-client, phe, pylibsrtp, cryptography, av, netifaces, aioice, pyee, crc32c, aiortc, protobuf, syft-proto, idna, requests, syft\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed Pillow-6.2.2 aioice-0.6.18 aiortc-0.9.28 av-8.0.1 crc32c-2.0 cryptography-2.9.2 flask-socketio-4.2.1 idna-2.8 lz4-3.0.2 netifaces-0.10.9 phe-1.4.0 protobuf-3.12.1 psutil-5.7.0 pyee-7.0.2 pylibsrtp-0.6.6 python-engineio-3.13.0 python-socketio-4.6.0 requests-2.22.0 syft-0.2.6 syft-proto-0.4.5 torch-1.4.0 torchvision-0.5.0 websocket-client-0.57.0 websockets-8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "idna",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo3Sj8A-uyTq",
        "colab_type": "text"
      },
      "source": [
        "### Remote Execution in PySyft\n",
        "\n",
        "The essence of Federated Learning is the ability to train models in parallel on a wide number of machines. Thus, we need the ability to tell remote machines to execute the operations required for Deep Learning.\n",
        "\n",
        "Thus, instead of using Torch tensors - we're now going to work with pointers to tensors. Say we are executing remotely on my machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TohfCqHOujFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy\n",
        "import torch\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG8lyRN3wKqs",
        "colab_type": "text"
      },
      "source": [
        "##### Properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGQIRLzCupXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "3b88f7d5-6bd7-46ec-88ea-31a9704655dc"
      },
      "source": [
        "gok = sy.VirtualWorker(hook, id=\"gok\")\n",
        "print(gok._objects)\n",
        "\n",
        "prof = sy.VirtualWorker(hook, id=\"prof\")\n",
        "print(gok._objects)\n",
        "\n",
        "\n",
        "x = torch.tensor([1,2,3,4,5])\n",
        "x = x.send(gok)\n",
        "gok._objects, x.location, x.id_at_location, x.id, x.owner, hook.local_worker, x"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{38196090708: tensor([[1., 1.],\n",
            "        [0., 1.]], requires_grad=True), 97787146248: tensor([[1.],\n",
            "        [1.]], requires_grad=True)}\n",
            "{38196090708: tensor([[1., 1.],\n",
            "        [0., 1.]], requires_grad=True), 97787146248: tensor([[1.],\n",
            "        [1.]], requires_grad=True)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({38196090708: tensor([[1., 1.],\n",
              "          [0., 1.]], requires_grad=True),\n",
              "  92021371233: tensor([1, 2, 3, 4, 5]),\n",
              "  97787146248: tensor([[1.],\n",
              "          [1.]], requires_grad=True)},\n",
              " <VirtualWorker id:gok #objects:3>,\n",
              " 92021371233,\n",
              " 82815723811,\n",
              " <VirtualWorker id:me #objects:0>,\n",
              " <VirtualWorker id:me #objects:0>,\n",
              " (Wrapper)>[PointerTensor | me:82815723811 -> gok:92021371233])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFjIpvICwCtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19501f39-d2b3-4d64-e691-7d0802ed72b1"
      },
      "source": [
        "x = x.get()\n",
        "gok._objects"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEFOUnCowT-1",
        "colab_type": "text"
      },
      "source": [
        "### Toy Learning\n",
        "\n",
        "Lets fit a model to distinguish simple linearly differentiable data \\\\\n",
        "Scenario 1: Server training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqYsLGpwJqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b7d09816-0de7-4ec8-c9db-c891a2e1b57b"
      },
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "# Toy Dataset\n",
        "data = torch.tensor([[1.,1],[0,1],[1,0],[0,0]], requires_grad=True)\n",
        "target = torch.tensor([[1.],[1], [0], [0]], requires_grad=True)\n",
        "\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "opt = optim.SGD(params=model.parameters(), lr=0.1)\n",
        "\n",
        "def train(iterations=20):\n",
        "\n",
        "    for iter in range(iterations):\n",
        "        opt.zero_grad()\n",
        "        pred = model(data)\n",
        "        loss = ((pred - target)**2).sum()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        print(loss.data)\n",
        "        \n",
        "train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.3795)\n",
            "tensor(2.0015)\n",
            "tensor(1.0764)\n",
            "tensor(0.6738)\n",
            "tensor(0.4303)\n",
            "tensor(0.2755)\n",
            "tensor(0.1765)\n",
            "tensor(0.1131)\n",
            "tensor(0.0725)\n",
            "tensor(0.0464)\n",
            "tensor(0.0298)\n",
            "tensor(0.0191)\n",
            "tensor(0.0123)\n",
            "tensor(0.0079)\n",
            "tensor(0.0051)\n",
            "tensor(0.0032)\n",
            "tensor(0.0021)\n",
            "tensor(0.0013)\n",
            "tensor(0.0009)\n",
            "tensor(0.0006)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x7GRuFUx5gJ",
        "colab_type": "text"
      },
      "source": [
        "Scenario 2: Remote On-devices training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxgK8ienxLfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "7dbbf865-01fa-4e54-9f55-e72124ff2374"
      },
      "source": [
        "data_gok = data[0:2].send(gok)\n",
        "target_gok = target[0:2].send(gok)\n",
        "\n",
        "data_prof = data[2:4].send(prof)\n",
        "target_prof = target[2:4].send(prof)\n",
        "\n",
        "\n",
        "datasets = [(data_gok, target_gok), (data_prof, target_prof)]\n",
        "\n",
        "def train(iterations=20):\n",
        "\n",
        "    model = nn.Linear(2,1)\n",
        "    opt = optim.SGD(params=model.parameters(), lr=0.1)\n",
        "    \n",
        "    for iter in range(iterations):\n",
        "\n",
        "        for _data, _target in datasets:\n",
        "\n",
        "            # send model to data on-device location\n",
        "            model = model.send(_data.location)\n",
        "\n",
        "            # do normal training\n",
        "            opt.zero_grad()\n",
        "            pred = model(_data)\n",
        "            loss = ((pred - _target)**2).sum()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # get smarter model back\n",
        "            model = model.get()\n",
        "\n",
        "            print(loss.get())\n",
        "            \n",
        "train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.0678, requires_grad=True)\n",
            "tensor(0.6259, requires_grad=True)\n",
            "tensor(0.5441, requires_grad=True)\n",
            "tensor(0.4290, requires_grad=True)\n",
            "tensor(0.2923, requires_grad=True)\n",
            "tensor(0.2539, requires_grad=True)\n",
            "tensor(0.1773, requires_grad=True)\n",
            "tensor(0.1501, requires_grad=True)\n",
            "tensor(0.1094, requires_grad=True)\n",
            "tensor(0.0898, requires_grad=True)\n",
            "tensor(0.0683, requires_grad=True)\n",
            "tensor(0.0545, requires_grad=True)\n",
            "tensor(0.0432, requires_grad=True)\n",
            "tensor(0.0337, requires_grad=True)\n",
            "tensor(0.0276, requires_grad=True)\n",
            "tensor(0.0212, requires_grad=True)\n",
            "tensor(0.0180, requires_grad=True)\n",
            "tensor(0.0137, requires_grad=True)\n",
            "tensor(0.0118, requires_grad=True)\n",
            "tensor(0.0091, requires_grad=True)\n",
            "tensor(0.0079, requires_grad=True)\n",
            "tensor(0.0062, requires_grad=True)\n",
            "tensor(0.0054, requires_grad=True)\n",
            "tensor(0.0043, requires_grad=True)\n",
            "tensor(0.0037, requires_grad=True)\n",
            "tensor(0.0030, requires_grad=True)\n",
            "tensor(0.0026, requires_grad=True)\n",
            "tensor(0.0022, requires_grad=True)\n",
            "tensor(0.0018, requires_grad=True)\n",
            "tensor(0.0016, requires_grad=True)\n",
            "tensor(0.0013, requires_grad=True)\n",
            "tensor(0.0012, requires_grad=True)\n",
            "tensor(0.0009, requires_grad=True)\n",
            "tensor(0.0009, requires_grad=True)\n",
            "tensor(0.0007, requires_grad=True)\n",
            "tensor(0.0006, requires_grad=True)\n",
            "tensor(0.0005, requires_grad=True)\n",
            "tensor(0.0005, requires_grad=True)\n",
            "tensor(0.0004, requires_grad=True)\n",
            "tensor(0.0004, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJLmTOXyzk0",
        "colab_type": "text"
      },
      "source": [
        "### Federated LEarning on Fashion MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0HjYufIxs6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71b7025c-035e-4820-fd32-0a5967f48e88"
      },
      "source": [
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch \n",
        "import syft as sy\n",
        "import copy\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VOW9_5UzHGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "c27c8b899d30496bad4507eaace9c8ee",
            "0093ad54b8764c34b1ed99038faca397",
            "80e20190685d4995bd21f702342d4a60",
            "9cffbb15f9fe40448ac98f2d1ac4e156",
            "ef178fdb09384eb29384bac312f711af",
            "59eff89a36cc4d09a27d0337a13ff03b",
            "542bec5bc7a84160ab97d5e80e5f5d4a",
            "8fa0dd7659874ff084b7c6d82215ea19",
            "559b9581d2984f4fbbc18088dfc2283d",
            "3869915b035445e2921db7489c60f8c6",
            "cacd01381afd4ffe88e17862faf941f5",
            "5e042e95ad994246bbfcb6dd6376ff9b",
            "2793360258b14103af2c586e975c0762",
            "791d64d351064b66a3274fe70ea06529",
            "2c5f759678aa47ee95e5d6da62b0e9b1",
            "5e85f08819a44aa0a703483e385e641b",
            "a4f22f9810d9446690ffcaf38e5aa149",
            "2159b20a463346d3bb475924ee15fbe6",
            "e13970df19c2485693127b4a65d2f4cb",
            "9466f30a7d22450281c2035973e36de0",
            "e3c5a5066e0f41529dff66f964175431",
            "d06978b6fece40c1b226941c1e38efcd",
            "50f3a1d940894b05a9bc1f577f1d4536",
            "15803bacc2974c9a8d211dc70d87e1c3",
            "ee42f928ca0b4aa9af9bddacf29aa813",
            "fe9588d7a0414c259e4f0a25ef58b7e4",
            "13264f0318da4c3ebfb4f490f773b883",
            "637a25c191b540dfaac9e631b377157f",
            "47f7ebfda11940d19c1977bad007888c",
            "22d915b925d64faa9305ea7d0ac1d74a",
            "3e08372168f5497dbf3965bff3b70372",
            "8a451682dfbf4b86a6db026931bdd02f"
          ]
        },
        "outputId": "e2bef1c8-410d-4cbf-f8d5-bd127bd3554e"
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5], [0.5])])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c27c8b899d30496bad4507eaace9c8ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "559b9581d2984f4fbbc18088dfc2283d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f22f9810d9446690ffcaf38e5aa149",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee42f928ca0b4aa9af9bddacf29aa813",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj_rkYOrz47d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "34d43025-fb14-487f-e892-91db58c57277"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image, label = next(iter(trainloader))\n",
        "plt.imshow(image[0,:][0]);\n",
        "print(label[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lR9iJV1ciS",
        "colab_type": "text"
      },
      "source": [
        "#### Custom Federated Learning dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPLPFjrf0rpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gok = sy.VirtualWorker(hook, id =\"gok\")\n",
        "prof = sy.VirtualWorker(hook, id=\"prof\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "def dataset_federate(dataset, workers):\n",
        "    logger.info(\"Scanning and sending data to {}...\".format(\", \".join([w.id for w in workers])))\n",
        "\n",
        "    data_size = math.ceil(len(dataset) / len(workers))\n",
        "\n",
        "    datasets = []\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=data_size, drop_last=True)\n",
        "    for dataset_idx, (data, targets) in enumerate(data_loader):\n",
        "        worker = workers[dataset_idx % len(workers)]\n",
        "        logger.debug(\"Sending data to worker %s\", worker.id)\n",
        "        data = data.send(worker)\n",
        "        targets = targets.send(worker)\n",
        "        datasets.append(sy.BaseDataset(data, targets))  # .send(worker)\n",
        "\n",
        "    logger.debug(\"Done!\")\n",
        "    return sy.FederatedDataset(datasets)\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    dataset_federate(\n",
        "        trainset,(gok, prof)\n",
        "        ), \n",
        "    batch_size=64, \n",
        "    shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7EwJRA61Y-E",
        "colab_type": "text"
      },
      "source": [
        "#### Model Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfKmjRs_0JCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "                    nn.Linear(784,128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128,64),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(64,10),\n",
        "                    nn.LogSoftmax(dim=1)\n",
        "                    )\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i39_iX261V5u",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGLRBNCs1LKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3df355d-649d-4e67-d76f-31813e9768d0"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
        "        data = data.view(data.shape[0], -1)\n",
        "        model.send(data.location)\n",
        "        output = model(data)\n",
        "        data, target = data.to('cpu'), target.to('cpu')\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get()\n",
        "        if batch_idx % 30 == 0:\n",
        "            loss = loss.get() \n",
        "            print('Train Epoch: {} Loss: {:.6f}'.format(\n",
        "                epoch, loss.item()))\n",
        "    model.eval()\n",
        "    print('Epoch: ', epoch)\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(testloader):\n",
        "        data = data.view(data.shape[0], -1)\n",
        "        output = model(data)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        print('Loss: ', loss.item())\n",
        "        max_arg_output = torch.argmax(output, dim=1)\n",
        "        total_correct += int(torch.sum(max_arg_output == target))\n",
        "        total += data.shape[0]\n",
        "    print('Testing data accuracy: {:.0%}'.format(total_correct/total))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 Loss: 2.295417\n",
            "Train Epoch: 0 Loss: 2.303848\n",
            "Train Epoch: 0 Loss: 2.304167\n",
            "Train Epoch: 0 Loss: 2.319456\n",
            "Train Epoch: 0 Loss: 2.303140\n",
            "Train Epoch: 0 Loss: 2.295700\n",
            "Train Epoch: 0 Loss: 2.280747\n",
            "Train Epoch: 0 Loss: 2.282676\n",
            "Train Epoch: 0 Loss: 2.246487\n",
            "Train Epoch: 0 Loss: 2.257097\n",
            "Train Epoch: 0 Loss: 2.258925\n",
            "Train Epoch: 0 Loss: 2.260521\n",
            "Train Epoch: 0 Loss: 2.253984\n",
            "Train Epoch: 0 Loss: 2.254073\n",
            "Train Epoch: 0 Loss: 2.229182\n",
            "Train Epoch: 0 Loss: 2.257481\n",
            "Train Epoch: 0 Loss: 2.231812\n",
            "Train Epoch: 0 Loss: 2.243870\n",
            "Train Epoch: 0 Loss: 2.219320\n",
            "Train Epoch: 0 Loss: 2.200479\n",
            "Train Epoch: 0 Loss: 2.223818\n",
            "Train Epoch: 0 Loss: 2.216469\n",
            "Train Epoch: 0 Loss: 2.209219\n",
            "Train Epoch: 0 Loss: 2.204651\n",
            "Train Epoch: 0 Loss: 2.185333\n",
            "Train Epoch: 0 Loss: 2.185384\n",
            "Train Epoch: 0 Loss: 2.183397\n",
            "Train Epoch: 0 Loss: 2.170432\n",
            "Train Epoch: 0 Loss: 2.162330\n",
            "Train Epoch: 0 Loss: 2.155714\n",
            "Train Epoch: 0 Loss: 2.156548\n",
            "Train Epoch: 0 Loss: 2.149725\n",
            "Epoch:  0\n",
            "Loss:  2.149935245513916\n",
            "Loss:  2.1631057262420654\n",
            "Loss:  2.1598031520843506\n",
            "Loss:  2.159393548965454\n",
            "Loss:  2.145935297012329\n",
            "Loss:  2.1455955505371094\n",
            "Loss:  2.159228563308716\n",
            "Loss:  2.152909994125366\n",
            "Loss:  2.1387557983398438\n",
            "Loss:  2.159200429916382\n",
            "Loss:  2.132780075073242\n",
            "Loss:  2.1611969470977783\n",
            "Loss:  2.154796600341797\n",
            "Loss:  2.1281180381774902\n",
            "Loss:  2.1674890518188477\n",
            "Loss:  2.143676519393921\n",
            "Loss:  2.159712553024292\n",
            "Loss:  2.1458630561828613\n",
            "Loss:  2.1395342350006104\n",
            "Loss:  2.1386971473693848\n",
            "Loss:  2.1506409645080566\n",
            "Loss:  2.1574840545654297\n",
            "Loss:  2.151395082473755\n",
            "Loss:  2.1418333053588867\n",
            "Loss:  2.138406753540039\n",
            "Loss:  2.129138946533203\n",
            "Loss:  2.1476798057556152\n",
            "Loss:  2.145388126373291\n",
            "Loss:  2.152151107788086\n",
            "Loss:  2.1575913429260254\n",
            "Loss:  2.140807628631592\n",
            "Loss:  2.1482608318328857\n",
            "Loss:  2.1393659114837646\n",
            "Loss:  2.154012441635132\n",
            "Loss:  2.1365807056427\n",
            "Loss:  2.1389496326446533\n",
            "Loss:  2.1167571544647217\n",
            "Loss:  2.150062084197998\n",
            "Loss:  2.1468935012817383\n",
            "Loss:  2.156294584274292\n",
            "Loss:  2.152782678604126\n",
            "Loss:  2.167527914047241\n",
            "Loss:  2.156259059906006\n",
            "Loss:  2.1398088932037354\n",
            "Loss:  2.154654026031494\n",
            "Loss:  2.1361441612243652\n",
            "Loss:  2.1380510330200195\n",
            "Loss:  2.16282320022583\n",
            "Loss:  2.1579151153564453\n",
            "Loss:  2.1299216747283936\n",
            "Loss:  2.145277261734009\n",
            "Loss:  2.1333603858947754\n",
            "Loss:  2.147094964981079\n",
            "Loss:  2.155439853668213\n",
            "Loss:  2.1416618824005127\n",
            "Loss:  2.135662078857422\n",
            "Loss:  2.1315174102783203\n",
            "Loss:  2.1578617095947266\n",
            "Loss:  2.163886547088623\n",
            "Loss:  2.1525778770446777\n",
            "Loss:  2.134291648864746\n",
            "Loss:  2.1376047134399414\n",
            "Loss:  2.1446478366851807\n",
            "Loss:  2.154661178588867\n",
            "Loss:  2.1464951038360596\n",
            "Loss:  2.141386032104492\n",
            "Loss:  2.1542909145355225\n",
            "Loss:  2.115741729736328\n",
            "Loss:  2.1554555892944336\n",
            "Loss:  2.1448569297790527\n",
            "Loss:  2.139005661010742\n",
            "Loss:  2.146099328994751\n",
            "Loss:  2.1435256004333496\n",
            "Loss:  2.1696548461914062\n",
            "Loss:  2.1536760330200195\n",
            "Loss:  2.1673495769500732\n",
            "Loss:  2.146303176879883\n",
            "Loss:  2.1544690132141113\n",
            "Loss:  2.129157543182373\n",
            "Loss:  2.142171621322632\n",
            "Loss:  2.143470287322998\n",
            "Loss:  2.1391122341156006\n",
            "Loss:  2.152127504348755\n",
            "Loss:  2.129162073135376\n",
            "Loss:  2.160651445388794\n",
            "Loss:  2.162147045135498\n",
            "Loss:  2.1303534507751465\n",
            "Loss:  2.162097692489624\n",
            "Loss:  2.135805130004883\n",
            "Loss:  2.142076015472412\n",
            "Loss:  2.135563850402832\n",
            "Loss:  2.1416823863983154\n",
            "Loss:  2.153543710708618\n",
            "Loss:  2.1445372104644775\n",
            "Loss:  2.143695831298828\n",
            "Loss:  2.162731885910034\n",
            "Loss:  2.16462779045105\n",
            "Loss:  2.1230430603027344\n",
            "Loss:  2.1369314193725586\n",
            "Loss:  2.153888702392578\n",
            "Loss:  2.139568328857422\n",
            "Loss:  2.1447150707244873\n",
            "Loss:  2.130413055419922\n",
            "Loss:  2.1453545093536377\n",
            "Loss:  2.1312930583953857\n",
            "Loss:  2.1316583156585693\n",
            "Loss:  2.160029411315918\n",
            "Loss:  2.151679277420044\n",
            "Loss:  2.1395061016082764\n",
            "Loss:  2.1499264240264893\n",
            "Loss:  2.1691513061523438\n",
            "Loss:  2.146338939666748\n",
            "Loss:  2.1483027935028076\n",
            "Loss:  2.141218900680542\n",
            "Loss:  2.1713547706604004\n",
            "Loss:  2.161242723464966\n",
            "Loss:  2.1516826152801514\n",
            "Loss:  2.1316442489624023\n",
            "Loss:  2.1448073387145996\n",
            "Loss:  2.1623644828796387\n",
            "Loss:  2.1252496242523193\n",
            "Loss:  2.1434149742126465\n",
            "Loss:  2.1586244106292725\n",
            "Loss:  2.150066614151001\n",
            "Loss:  2.1598856449127197\n",
            "Loss:  2.162003517150879\n",
            "Loss:  2.124925136566162\n",
            "Loss:  2.170168161392212\n",
            "Loss:  2.181889533996582\n",
            "Loss:  2.1289777755737305\n",
            "Loss:  2.1512207984924316\n",
            "Loss:  2.140491485595703\n",
            "Loss:  2.1504015922546387\n",
            "Loss:  2.1431691646575928\n",
            "Loss:  2.1444814205169678\n",
            "Loss:  2.112504482269287\n",
            "Loss:  2.129077672958374\n",
            "Loss:  2.168346881866455\n",
            "Loss:  2.1496670246124268\n",
            "Loss:  2.1443521976470947\n",
            "Loss:  2.1454029083251953\n",
            "Loss:  2.1239371299743652\n",
            "Loss:  2.1674258708953857\n",
            "Loss:  2.163480520248413\n",
            "Loss:  2.1508126258850098\n",
            "Loss:  2.148756504058838\n",
            "Loss:  2.131502151489258\n",
            "Loss:  2.1629388332366943\n",
            "Loss:  2.169536590576172\n",
            "Loss:  2.1391444206237793\n",
            "Loss:  2.149231433868408\n",
            "Loss:  2.163280487060547\n",
            "Loss:  2.1533408164978027\n",
            "Loss:  2.1324539184570312\n",
            "Loss:  2.156773567199707\n",
            "Loss:  2.1502797603607178\n",
            "Loss:  2.170705556869507\n",
            "Testing data accuracy: 39%\n",
            "Train Epoch: 1 Loss: 2.158797\n",
            "Train Epoch: 1 Loss: 2.153712\n",
            "Train Epoch: 1 Loss: 2.138382\n",
            "Train Epoch: 1 Loss: 2.127007\n",
            "Train Epoch: 1 Loss: 2.153612\n",
            "Train Epoch: 1 Loss: 2.141519\n",
            "Train Epoch: 1 Loss: 2.108663\n",
            "Train Epoch: 1 Loss: 2.086532\n",
            "Train Epoch: 1 Loss: 2.097087\n",
            "Train Epoch: 1 Loss: 2.080130\n",
            "Train Epoch: 1 Loss: 2.076772\n",
            "Train Epoch: 1 Loss: 2.057108\n",
            "Train Epoch: 1 Loss: 2.051769\n",
            "Train Epoch: 1 Loss: 2.032795\n",
            "Train Epoch: 1 Loss: 2.056132\n",
            "Train Epoch: 1 Loss: 2.035645\n",
            "Train Epoch: 1 Loss: 2.057172\n",
            "Train Epoch: 1 Loss: 1.999234\n",
            "Train Epoch: 1 Loss: 2.027121\n",
            "Train Epoch: 1 Loss: 2.026197\n",
            "Train Epoch: 1 Loss: 2.007007\n",
            "Train Epoch: 1 Loss: 1.955700\n",
            "Train Epoch: 1 Loss: 1.976825\n",
            "Train Epoch: 1 Loss: 1.944631\n",
            "Train Epoch: 1 Loss: 1.993220\n",
            "Train Epoch: 1 Loss: 1.933952\n",
            "Train Epoch: 1 Loss: 1.964723\n",
            "Train Epoch: 1 Loss: 1.979174\n",
            "Train Epoch: 1 Loss: 1.912369\n",
            "Train Epoch: 1 Loss: 1.958608\n",
            "Train Epoch: 1 Loss: 1.889683\n",
            "Train Epoch: 1 Loss: 1.852028\n",
            "Epoch:  1\n",
            "Loss:  1.9480654001235962\n",
            "Loss:  1.9389381408691406\n",
            "Loss:  1.8736752271652222\n",
            "Loss:  1.919738531112671\n",
            "Loss:  1.9303253889083862\n",
            "Loss:  1.904729962348938\n",
            "Loss:  1.9278135299682617\n",
            "Loss:  1.9024914503097534\n",
            "Loss:  1.850181221961975\n",
            "Loss:  1.8356138467788696\n",
            "Loss:  1.9148039817810059\n",
            "Loss:  1.9218205213546753\n",
            "Loss:  1.886413335800171\n",
            "Loss:  1.8829846382141113\n",
            "Loss:  1.869443416595459\n",
            "Loss:  1.9625548124313354\n",
            "Loss:  1.8979276418685913\n",
            "Loss:  1.894601583480835\n",
            "Loss:  1.8954722881317139\n",
            "Loss:  1.8912193775177002\n",
            "Loss:  1.8636949062347412\n",
            "Loss:  1.8848023414611816\n",
            "Loss:  1.9079762697219849\n",
            "Loss:  1.9213879108428955\n",
            "Loss:  1.9153887033462524\n",
            "Loss:  1.8611786365509033\n",
            "Loss:  1.8593422174453735\n",
            "Loss:  1.9148180484771729\n",
            "Loss:  1.8681004047393799\n",
            "Loss:  1.9060847759246826\n",
            "Loss:  1.925925374031067\n",
            "Loss:  1.901961088180542\n",
            "Loss:  1.9271799325942993\n",
            "Loss:  1.9303985834121704\n",
            "Loss:  2.0008134841918945\n",
            "Loss:  1.9259047508239746\n",
            "Loss:  1.901233434677124\n",
            "Loss:  1.8944984674453735\n",
            "Loss:  1.8724420070648193\n",
            "Loss:  1.8788961172103882\n",
            "Loss:  1.9425783157348633\n",
            "Loss:  1.9024795293807983\n",
            "Loss:  1.9700227975845337\n",
            "Loss:  1.8942168951034546\n",
            "Loss:  1.8844572305679321\n",
            "Loss:  1.902777075767517\n",
            "Loss:  1.946730375289917\n",
            "Loss:  1.8674557209014893\n",
            "Loss:  1.8872889280319214\n",
            "Loss:  1.9220561981201172\n",
            "Loss:  1.9229135513305664\n",
            "Loss:  1.8850429058074951\n",
            "Loss:  1.8851889371871948\n",
            "Loss:  1.8590656518936157\n",
            "Loss:  1.8860251903533936\n",
            "Loss:  1.8956716060638428\n",
            "Loss:  1.8512290716171265\n",
            "Loss:  1.8964595794677734\n",
            "Loss:  1.8769029378890991\n",
            "Loss:  1.8962661027908325\n",
            "Loss:  1.8553125858306885\n",
            "Loss:  1.8973733186721802\n",
            "Loss:  1.8750464916229248\n",
            "Loss:  1.9285029172897339\n",
            "Loss:  1.869349479675293\n",
            "Loss:  1.914765477180481\n",
            "Loss:  1.9229387044906616\n",
            "Loss:  1.8803248405456543\n",
            "Loss:  1.8623509407043457\n",
            "Loss:  1.9095343351364136\n",
            "Loss:  1.9328416585922241\n",
            "Loss:  1.9038549661636353\n",
            "Loss:  1.8865268230438232\n",
            "Loss:  1.8940075635910034\n",
            "Loss:  1.903208613395691\n",
            "Loss:  1.894295573234558\n",
            "Loss:  1.8449620008468628\n",
            "Loss:  1.846826434135437\n",
            "Loss:  1.914199709892273\n",
            "Loss:  1.8843674659729004\n",
            "Loss:  1.913185954093933\n",
            "Loss:  1.9429782629013062\n",
            "Loss:  1.8601456880569458\n",
            "Loss:  1.917829990386963\n",
            "Loss:  1.8940366506576538\n",
            "Loss:  1.9083877801895142\n",
            "Loss:  1.857785701751709\n",
            "Loss:  1.9031068086624146\n",
            "Loss:  1.9103504419326782\n",
            "Loss:  1.873853087425232\n",
            "Loss:  1.9161204099655151\n",
            "Loss:  1.8588041067123413\n",
            "Loss:  1.9301546812057495\n",
            "Loss:  1.8808938264846802\n",
            "Loss:  1.9370300769805908\n",
            "Loss:  1.9205173254013062\n",
            "Loss:  1.8661739826202393\n",
            "Loss:  1.9045789241790771\n",
            "Loss:  1.9051921367645264\n",
            "Loss:  1.877898097038269\n",
            "Loss:  1.8755743503570557\n",
            "Loss:  1.8883147239685059\n",
            "Loss:  1.9497966766357422\n",
            "Loss:  1.88657546043396\n",
            "Loss:  1.8210957050323486\n",
            "Loss:  1.8513506650924683\n",
            "Loss:  1.8502272367477417\n",
            "Loss:  1.9066786766052246\n",
            "Loss:  1.902771234512329\n",
            "Loss:  1.8657128810882568\n",
            "Loss:  1.8927463293075562\n",
            "Loss:  1.8231940269470215\n",
            "Loss:  1.9144477844238281\n",
            "Loss:  1.8644945621490479\n",
            "Loss:  1.9265902042388916\n",
            "Loss:  1.8884990215301514\n",
            "Loss:  1.9081335067749023\n",
            "Loss:  1.9225438833236694\n",
            "Loss:  1.8849544525146484\n",
            "Loss:  1.8860067129135132\n",
            "Loss:  1.947202205657959\n",
            "Loss:  1.8687728643417358\n",
            "Loss:  1.8516080379486084\n",
            "Loss:  1.9192811250686646\n",
            "Loss:  1.9231284856796265\n",
            "Loss:  1.9386972188949585\n",
            "Loss:  1.9274024963378906\n",
            "Loss:  1.8701709508895874\n",
            "Loss:  1.9387263059616089\n",
            "Loss:  1.889209270477295\n",
            "Loss:  1.914729356765747\n",
            "Loss:  1.9227951765060425\n",
            "Loss:  1.895103931427002\n",
            "Loss:  1.8786622285842896\n",
            "Loss:  1.9080333709716797\n",
            "Loss:  1.8880034685134888\n",
            "Loss:  1.8937188386917114\n",
            "Loss:  1.8588975667953491\n",
            "Loss:  1.8582020998001099\n",
            "Loss:  1.848874568939209\n",
            "Loss:  1.8871080875396729\n",
            "Loss:  1.9149035215377808\n",
            "Loss:  1.8757327795028687\n",
            "Loss:  1.9043883085250854\n",
            "Loss:  1.9383429288864136\n",
            "Loss:  1.9527642726898193\n",
            "Loss:  1.9088414907455444\n",
            "Loss:  1.8728257417678833\n",
            "Loss:  1.857433795928955\n",
            "Loss:  1.890467882156372\n",
            "Loss:  1.897929310798645\n",
            "Loss:  1.8981143236160278\n",
            "Loss:  1.9112848043441772\n",
            "Loss:  1.8965115547180176\n",
            "Loss:  1.908660888671875\n",
            "Loss:  1.888466715812683\n",
            "Loss:  1.8571375608444214\n",
            "Testing data accuracy: 48%\n",
            "Train Epoch: 2 Loss: 1.918725\n",
            "Train Epoch: 2 Loss: 1.862796\n",
            "Train Epoch: 2 Loss: 1.900085\n",
            "Train Epoch: 2 Loss: 1.917095\n",
            "Train Epoch: 2 Loss: 1.866480\n",
            "Train Epoch: 2 Loss: 1.798915\n",
            "Train Epoch: 2 Loss: 1.769868\n",
            "Train Epoch: 2 Loss: 1.835477\n",
            "Train Epoch: 2 Loss: 1.804579\n",
            "Train Epoch: 2 Loss: 1.780643\n",
            "Train Epoch: 2 Loss: 1.817428\n",
            "Train Epoch: 2 Loss: 1.794443\n",
            "Train Epoch: 2 Loss: 1.791594\n",
            "Train Epoch: 2 Loss: 1.804077\n",
            "Train Epoch: 2 Loss: 1.791346\n",
            "Train Epoch: 2 Loss: 1.720217\n",
            "Train Epoch: 2 Loss: 1.735247\n",
            "Train Epoch: 2 Loss: 1.711125\n",
            "Train Epoch: 2 Loss: 1.684113\n",
            "Train Epoch: 2 Loss: 1.658410\n",
            "Train Epoch: 2 Loss: 1.714455\n",
            "Train Epoch: 2 Loss: 1.722951\n",
            "Train Epoch: 2 Loss: 1.678510\n",
            "Train Epoch: 2 Loss: 1.544942\n",
            "Train Epoch: 2 Loss: 1.659745\n",
            "Train Epoch: 2 Loss: 1.618173\n",
            "Train Epoch: 2 Loss: 1.727676\n",
            "Train Epoch: 2 Loss: 1.643998\n",
            "Train Epoch: 2 Loss: 1.523897\n",
            "Train Epoch: 2 Loss: 1.538368\n",
            "Train Epoch: 2 Loss: 1.584768\n",
            "Train Epoch: 2 Loss: 1.662731\n",
            "Epoch:  2\n",
            "Loss:  1.5808990001678467\n",
            "Loss:  1.563367486000061\n",
            "Loss:  1.6241496801376343\n",
            "Loss:  1.6131176948547363\n",
            "Loss:  1.629953384399414\n",
            "Loss:  1.600469946861267\n",
            "Loss:  1.5386853218078613\n",
            "Loss:  1.5096865892410278\n",
            "Loss:  1.6049197912216187\n",
            "Loss:  1.5152158737182617\n",
            "Loss:  1.5597255229949951\n",
            "Loss:  1.4585130214691162\n",
            "Loss:  1.5892808437347412\n",
            "Loss:  1.5437531471252441\n",
            "Loss:  1.6890239715576172\n",
            "Loss:  1.5534378290176392\n",
            "Loss:  1.6526412963867188\n",
            "Loss:  1.5765621662139893\n",
            "Loss:  1.4942703247070312\n",
            "Loss:  1.6010035276412964\n",
            "Loss:  1.5560517311096191\n",
            "Loss:  1.6041604280471802\n",
            "Loss:  1.5965502262115479\n",
            "Loss:  1.5918265581130981\n",
            "Loss:  1.5944634675979614\n",
            "Loss:  1.4630439281463623\n",
            "Loss:  1.5419018268585205\n",
            "Loss:  1.5068084001541138\n",
            "Loss:  1.617177128791809\n",
            "Loss:  1.6162559986114502\n",
            "Loss:  1.4998745918273926\n",
            "Loss:  1.5731676816940308\n",
            "Loss:  1.6604313850402832\n",
            "Loss:  1.6626968383789062\n",
            "Loss:  1.5968737602233887\n",
            "Loss:  1.6229280233383179\n",
            "Loss:  1.5744708776474\n",
            "Loss:  1.6139872074127197\n",
            "Loss:  1.5300559997558594\n",
            "Loss:  1.65073561668396\n",
            "Loss:  1.5492374897003174\n",
            "Loss:  1.6063811779022217\n",
            "Loss:  1.5733689069747925\n",
            "Loss:  1.5962847471237183\n",
            "Loss:  1.6241518259048462\n",
            "Loss:  1.53366219997406\n",
            "Loss:  1.6235711574554443\n",
            "Loss:  1.5990077257156372\n",
            "Loss:  1.6088674068450928\n",
            "Loss:  1.6014975309371948\n",
            "Loss:  1.598418116569519\n",
            "Loss:  1.5859296321868896\n",
            "Loss:  1.566217303276062\n",
            "Loss:  1.6481728553771973\n",
            "Loss:  1.618741512298584\n",
            "Loss:  1.5863852500915527\n",
            "Loss:  1.5828157663345337\n",
            "Loss:  1.5555521249771118\n",
            "Loss:  1.5906347036361694\n",
            "Loss:  1.6164356470108032\n",
            "Loss:  1.6261868476867676\n",
            "Loss:  1.5601097345352173\n",
            "Loss:  1.6002188920974731\n",
            "Loss:  1.6339306831359863\n",
            "Loss:  1.581847071647644\n",
            "Loss:  1.560863971710205\n",
            "Loss:  1.6205005645751953\n",
            "Loss:  1.6468098163604736\n",
            "Loss:  1.6097179651260376\n",
            "Loss:  1.5659843683242798\n",
            "Loss:  1.614096999168396\n",
            "Loss:  1.5940096378326416\n",
            "Loss:  1.5744658708572388\n",
            "Loss:  1.5125705003738403\n",
            "Loss:  1.6170525550842285\n",
            "Loss:  1.5000518560409546\n",
            "Loss:  1.6789255142211914\n",
            "Loss:  1.5676512718200684\n",
            "Loss:  1.653293490409851\n",
            "Loss:  1.633359670639038\n",
            "Loss:  1.5815823078155518\n",
            "Loss:  1.5738393068313599\n",
            "Loss:  1.64336097240448\n",
            "Loss:  1.6139508485794067\n",
            "Loss:  1.6430479288101196\n",
            "Loss:  1.5387098789215088\n",
            "Loss:  1.6644350290298462\n",
            "Loss:  1.5740571022033691\n",
            "Loss:  1.5215646028518677\n",
            "Loss:  1.4983659982681274\n",
            "Loss:  1.6059294939041138\n",
            "Loss:  1.5870673656463623\n",
            "Loss:  1.5925824642181396\n",
            "Loss:  1.6566457748413086\n",
            "Loss:  1.6015366315841675\n",
            "Loss:  1.609363317489624\n",
            "Loss:  1.5645018815994263\n",
            "Loss:  1.5712523460388184\n",
            "Loss:  1.5399338006973267\n",
            "Loss:  1.60068941116333\n",
            "Loss:  1.6469537019729614\n",
            "Loss:  1.609004020690918\n",
            "Loss:  1.632235050201416\n",
            "Loss:  1.6467187404632568\n",
            "Loss:  1.5897581577301025\n",
            "Loss:  1.6427732706069946\n",
            "Loss:  1.5635979175567627\n",
            "Loss:  1.5784714221954346\n",
            "Loss:  1.5304999351501465\n",
            "Loss:  1.5899817943572998\n",
            "Loss:  1.596013069152832\n",
            "Loss:  1.5258647203445435\n",
            "Loss:  1.5504306554794312\n",
            "Loss:  1.5577921867370605\n",
            "Loss:  1.5554689168930054\n",
            "Loss:  1.6292295455932617\n",
            "Loss:  1.6226357221603394\n",
            "Loss:  1.6706440448760986\n",
            "Loss:  1.5688432455062866\n",
            "Loss:  1.581222414970398\n",
            "Loss:  1.625188946723938\n",
            "Loss:  1.6200937032699585\n",
            "Loss:  1.6392884254455566\n",
            "Loss:  1.5845123529434204\n",
            "Loss:  1.6198930740356445\n",
            "Loss:  1.6012592315673828\n",
            "Loss:  1.540001392364502\n",
            "Loss:  1.6079518795013428\n",
            "Loss:  1.5619288682937622\n",
            "Loss:  1.6365901231765747\n",
            "Loss:  1.5686291456222534\n",
            "Loss:  1.5083173513412476\n",
            "Loss:  1.6208230257034302\n",
            "Loss:  1.494268536567688\n",
            "Loss:  1.7109054327011108\n",
            "Loss:  1.6024562120437622\n",
            "Loss:  1.5538815259933472\n",
            "Loss:  1.6283822059631348\n",
            "Loss:  1.6146857738494873\n",
            "Loss:  1.565633773803711\n",
            "Loss:  1.559933066368103\n",
            "Loss:  1.6219651699066162\n",
            "Loss:  1.6324995756149292\n",
            "Loss:  1.6837224960327148\n",
            "Loss:  1.5820906162261963\n",
            "Loss:  1.5295594930648804\n",
            "Loss:  1.5503119230270386\n",
            "Loss:  1.639003038406372\n",
            "Loss:  1.6161525249481201\n",
            "Loss:  1.5582551956176758\n",
            "Loss:  1.584436058998108\n",
            "Loss:  1.568806767463684\n",
            "Loss:  1.5829296112060547\n",
            "Loss:  1.571677803993225\n",
            "Loss:  1.6544626951217651\n",
            "Loss:  1.5352468490600586\n",
            "Loss:  1.496133804321289\n",
            "Testing data accuracy: 53%\n",
            "Train Epoch: 3 Loss: 1.579100\n",
            "Train Epoch: 3 Loss: 1.714606\n",
            "Train Epoch: 3 Loss: 1.566492\n",
            "Train Epoch: 3 Loss: 1.557529\n",
            "Train Epoch: 3 Loss: 1.633014\n",
            "Train Epoch: 3 Loss: 1.489300\n",
            "Train Epoch: 3 Loss: 1.527253\n",
            "Train Epoch: 3 Loss: 1.521771\n",
            "Train Epoch: 3 Loss: 1.470158\n",
            "Train Epoch: 3 Loss: 1.504321\n",
            "Train Epoch: 3 Loss: 1.519168\n",
            "Train Epoch: 3 Loss: 1.450689\n",
            "Train Epoch: 3 Loss: 1.582761\n",
            "Train Epoch: 3 Loss: 1.555287\n",
            "Train Epoch: 3 Loss: 1.449400\n",
            "Train Epoch: 3 Loss: 1.471501\n",
            "Train Epoch: 3 Loss: 1.426621\n",
            "Train Epoch: 3 Loss: 1.439272\n",
            "Train Epoch: 3 Loss: 1.435520\n",
            "Train Epoch: 3 Loss: 1.341957\n",
            "Train Epoch: 3 Loss: 1.535155\n",
            "Train Epoch: 3 Loss: 1.401153\n",
            "Train Epoch: 3 Loss: 1.356354\n",
            "Train Epoch: 3 Loss: 1.379696\n",
            "Train Epoch: 3 Loss: 1.381974\n",
            "Train Epoch: 3 Loss: 1.393563\n",
            "Train Epoch: 3 Loss: 1.344870\n",
            "Train Epoch: 3 Loss: 1.479114\n",
            "Train Epoch: 3 Loss: 1.338872\n",
            "Train Epoch: 3 Loss: 1.415114\n",
            "Train Epoch: 3 Loss: 1.380898\n",
            "Train Epoch: 3 Loss: 1.318594\n",
            "Epoch:  3\n",
            "Loss:  1.3335856199264526\n",
            "Loss:  1.2259821891784668\n",
            "Loss:  1.3543275594711304\n",
            "Loss:  1.379976511001587\n",
            "Loss:  1.3158084154129028\n",
            "Loss:  1.2371940612792969\n",
            "Loss:  1.3133472204208374\n",
            "Loss:  1.4070719480514526\n",
            "Loss:  1.2851548194885254\n",
            "Loss:  1.3658537864685059\n",
            "Loss:  1.3239721059799194\n",
            "Loss:  1.2300405502319336\n",
            "Loss:  1.3557238578796387\n",
            "Loss:  1.365645408630371\n",
            "Loss:  1.3214445114135742\n",
            "Loss:  1.3325825929641724\n",
            "Loss:  1.4265015125274658\n",
            "Loss:  1.2737866640090942\n",
            "Loss:  1.337749719619751\n",
            "Loss:  1.2620338201522827\n",
            "Loss:  1.4351563453674316\n",
            "Loss:  1.35557222366333\n",
            "Loss:  1.4714685678482056\n",
            "Loss:  1.3719063997268677\n",
            "Loss:  1.3616591691970825\n",
            "Loss:  1.2907880544662476\n",
            "Loss:  1.3694502115249634\n",
            "Loss:  1.434090256690979\n",
            "Loss:  1.263691782951355\n",
            "Loss:  1.239438772201538\n",
            "Loss:  1.3632134199142456\n",
            "Loss:  1.2806066274642944\n",
            "Loss:  1.3589377403259277\n",
            "Loss:  1.4344977140426636\n",
            "Loss:  1.3348913192749023\n",
            "Loss:  1.2852824926376343\n",
            "Loss:  1.312559962272644\n",
            "Loss:  1.28072988986969\n",
            "Loss:  1.3535714149475098\n",
            "Loss:  1.4561914205551147\n",
            "Loss:  1.38275945186615\n",
            "Loss:  1.2376230955123901\n",
            "Loss:  1.424056887626648\n",
            "Loss:  1.3069618940353394\n",
            "Loss:  1.4097007513046265\n",
            "Loss:  1.332223892211914\n",
            "Loss:  1.2709766626358032\n",
            "Loss:  1.3555127382278442\n",
            "Loss:  1.295989990234375\n",
            "Loss:  1.463263988494873\n",
            "Loss:  1.4625507593154907\n",
            "Loss:  1.3626247644424438\n",
            "Loss:  1.2896912097930908\n",
            "Loss:  1.3902291059494019\n",
            "Loss:  1.3986681699752808\n",
            "Loss:  1.409942388534546\n",
            "Loss:  1.4452053308486938\n",
            "Loss:  1.2884771823883057\n",
            "Loss:  1.2242398262023926\n",
            "Loss:  1.4581246376037598\n",
            "Loss:  1.291716456413269\n",
            "Loss:  1.2930272817611694\n",
            "Loss:  1.343909502029419\n",
            "Loss:  1.3224759101867676\n",
            "Loss:  1.4131827354431152\n",
            "Loss:  1.2867807149887085\n",
            "Loss:  1.3224354982376099\n",
            "Loss:  1.2865917682647705\n",
            "Loss:  1.4049595594406128\n",
            "Loss:  1.3461601734161377\n",
            "Loss:  1.3841863870620728\n",
            "Loss:  1.2480957508087158\n",
            "Loss:  1.2431535720825195\n",
            "Loss:  1.373952031135559\n",
            "Loss:  1.325453519821167\n",
            "Loss:  1.3312379121780396\n",
            "Loss:  1.348433256149292\n",
            "Loss:  1.3568750619888306\n",
            "Loss:  1.3757075071334839\n",
            "Loss:  1.3175883293151855\n",
            "Loss:  1.3049437999725342\n",
            "Loss:  1.2979347705841064\n",
            "Loss:  1.2995084524154663\n",
            "Loss:  1.3667651414871216\n",
            "Loss:  1.4379746913909912\n",
            "Loss:  1.3284422159194946\n",
            "Loss:  1.3939340114593506\n",
            "Loss:  1.2712469100952148\n",
            "Loss:  1.3188583850860596\n",
            "Loss:  1.2957789897918701\n",
            "Loss:  1.3163868188858032\n",
            "Loss:  1.379024624824524\n",
            "Loss:  1.273046612739563\n",
            "Loss:  1.382083535194397\n",
            "Loss:  1.3525035381317139\n",
            "Loss:  1.3539561033248901\n",
            "Loss:  1.4523752927780151\n",
            "Loss:  1.2481224536895752\n",
            "Loss:  1.3456693887710571\n",
            "Loss:  1.4189302921295166\n",
            "Loss:  1.4600704908370972\n",
            "Loss:  1.3430842161178589\n",
            "Loss:  1.3744317293167114\n",
            "Loss:  1.2822399139404297\n",
            "Loss:  1.3850761651992798\n",
            "Loss:  1.4174726009368896\n",
            "Loss:  1.3880186080932617\n",
            "Loss:  1.3931653499603271\n",
            "Loss:  1.272990107536316\n",
            "Loss:  1.3414610624313354\n",
            "Loss:  1.3124816417694092\n",
            "Loss:  1.2835004329681396\n",
            "Loss:  1.3333635330200195\n",
            "Loss:  1.4204907417297363\n",
            "Loss:  1.3711097240447998\n",
            "Loss:  1.2909843921661377\n",
            "Loss:  1.4091864824295044\n",
            "Loss:  1.2841899394989014\n",
            "Loss:  1.3014590740203857\n",
            "Loss:  1.3373945951461792\n",
            "Loss:  1.309648036956787\n",
            "Loss:  1.3426238298416138\n",
            "Loss:  1.3949363231658936\n",
            "Loss:  1.3246674537658691\n",
            "Loss:  1.4316792488098145\n",
            "Loss:  1.3070793151855469\n",
            "Loss:  1.3653675317764282\n",
            "Loss:  1.358875036239624\n",
            "Loss:  1.4314846992492676\n",
            "Loss:  1.3611705303192139\n",
            "Loss:  1.3612977266311646\n",
            "Loss:  1.3544939756393433\n",
            "Loss:  1.2403984069824219\n",
            "Loss:  1.2167422771453857\n",
            "Loss:  1.3967236280441284\n",
            "Loss:  1.4216651916503906\n",
            "Loss:  1.2238558530807495\n",
            "Loss:  1.3262723684310913\n",
            "Loss:  1.335271954536438\n",
            "Loss:  1.3346561193466187\n",
            "Loss:  1.3260334730148315\n",
            "Loss:  1.4153406620025635\n",
            "Loss:  1.30379319190979\n",
            "Loss:  1.3355718851089478\n",
            "Loss:  1.2741585969924927\n",
            "Loss:  1.4063538312911987\n",
            "Loss:  1.3097330331802368\n",
            "Loss:  1.3051897287368774\n",
            "Loss:  1.347406268119812\n",
            "Loss:  1.4535473585128784\n",
            "Loss:  1.420867919921875\n",
            "Loss:  1.39201819896698\n",
            "Loss:  1.297075629234314\n",
            "Loss:  1.4062647819519043\n",
            "Loss:  1.29873788356781\n",
            "Loss:  1.3074390888214111\n",
            "Loss:  1.4811140298843384\n",
            "Testing data accuracy: 62%\n",
            "Train Epoch: 4 Loss: 1.320763\n",
            "Train Epoch: 4 Loss: 1.300690\n",
            "Train Epoch: 4 Loss: 1.392202\n",
            "Train Epoch: 4 Loss: 1.276141\n",
            "Train Epoch: 4 Loss: 1.451621\n",
            "Train Epoch: 4 Loss: 1.361267\n",
            "Train Epoch: 4 Loss: 1.333110\n",
            "Train Epoch: 4 Loss: 1.267670\n",
            "Train Epoch: 4 Loss: 1.220166\n",
            "Train Epoch: 4 Loss: 1.218952\n",
            "Train Epoch: 4 Loss: 1.299665\n",
            "Train Epoch: 4 Loss: 1.317989\n",
            "Train Epoch: 4 Loss: 1.214722\n",
            "Train Epoch: 4 Loss: 1.335703\n",
            "Train Epoch: 4 Loss: 1.373949\n",
            "Train Epoch: 4 Loss: 1.222277\n",
            "Train Epoch: 4 Loss: 1.261151\n",
            "Train Epoch: 4 Loss: 1.244819\n",
            "Train Epoch: 4 Loss: 1.212589\n",
            "Train Epoch: 4 Loss: 1.336449\n",
            "Train Epoch: 4 Loss: 1.086211\n",
            "Train Epoch: 4 Loss: 1.102630\n",
            "Train Epoch: 4 Loss: 1.209305\n",
            "Train Epoch: 4 Loss: 1.085758\n",
            "Train Epoch: 4 Loss: 1.167082\n",
            "Train Epoch: 4 Loss: 1.264481\n",
            "Train Epoch: 4 Loss: 1.178729\n",
            "Train Epoch: 4 Loss: 1.142046\n",
            "Train Epoch: 4 Loss: 1.212338\n",
            "Train Epoch: 4 Loss: 1.245304\n",
            "Train Epoch: 4 Loss: 1.149873\n",
            "Train Epoch: 4 Loss: 1.012295\n",
            "Epoch:  4\n",
            "Loss:  1.1916203498840332\n",
            "Loss:  1.306443691253662\n",
            "Loss:  1.1067943572998047\n",
            "Loss:  1.1949641704559326\n",
            "Loss:  1.0990142822265625\n",
            "Loss:  1.1840568780899048\n",
            "Loss:  1.2622939348220825\n",
            "Loss:  1.1131579875946045\n",
            "Loss:  1.1645375490188599\n",
            "Loss:  1.1570649147033691\n",
            "Loss:  1.115054965019226\n",
            "Loss:  1.0484317541122437\n",
            "Loss:  1.1617581844329834\n",
            "Loss:  1.097680687904358\n",
            "Loss:  1.0671741962432861\n",
            "Loss:  1.1390053033828735\n",
            "Loss:  1.2305724620819092\n",
            "Loss:  1.2884575128555298\n",
            "Loss:  1.164525032043457\n",
            "Loss:  1.1551252603530884\n",
            "Loss:  1.1359556913375854\n",
            "Loss:  1.1472456455230713\n",
            "Loss:  1.1881059408187866\n",
            "Loss:  1.1301614046096802\n",
            "Loss:  1.1057100296020508\n",
            "Loss:  1.18411123752594\n",
            "Loss:  1.078941822052002\n",
            "Loss:  1.1786774396896362\n",
            "Loss:  1.3890845775604248\n",
            "Loss:  1.1841578483581543\n",
            "Loss:  1.0500885248184204\n",
            "Loss:  1.196730136871338\n",
            "Loss:  1.090742826461792\n",
            "Loss:  1.0450786352157593\n",
            "Loss:  1.1041406393051147\n",
            "Loss:  1.2350491285324097\n",
            "Loss:  1.2860363721847534\n",
            "Loss:  1.282930612564087\n",
            "Loss:  1.23145592212677\n",
            "Loss:  1.064281702041626\n",
            "Loss:  1.2614593505859375\n",
            "Loss:  1.0736656188964844\n",
            "Loss:  1.126674771308899\n",
            "Loss:  1.0887246131896973\n",
            "Loss:  1.1778290271759033\n",
            "Loss:  1.292966604232788\n",
            "Loss:  1.15299654006958\n",
            "Loss:  1.1372166872024536\n",
            "Loss:  1.1688469648361206\n",
            "Loss:  1.3356553316116333\n",
            "Loss:  1.1235129833221436\n",
            "Loss:  1.209879755973816\n",
            "Loss:  1.0674631595611572\n",
            "Loss:  1.246822714805603\n",
            "Loss:  1.1982132196426392\n",
            "Loss:  1.2060192823410034\n",
            "Loss:  1.2339766025543213\n",
            "Loss:  1.1851840019226074\n",
            "Loss:  1.2174047231674194\n",
            "Loss:  1.1533015966415405\n",
            "Loss:  1.0650509595870972\n",
            "Loss:  1.0605887174606323\n",
            "Loss:  1.1321278810501099\n",
            "Loss:  1.1308996677398682\n",
            "Loss:  1.1301625967025757\n",
            "Loss:  1.1708723306655884\n",
            "Loss:  1.146925687789917\n",
            "Loss:  1.25240957736969\n",
            "Loss:  1.1509771347045898\n",
            "Loss:  1.19527268409729\n",
            "Loss:  1.1998146772384644\n",
            "Loss:  1.2185717821121216\n",
            "Loss:  1.1509644985198975\n",
            "Loss:  1.191510558128357\n",
            "Loss:  1.1655536890029907\n",
            "Loss:  1.1958190202713013\n",
            "Loss:  1.1760907173156738\n",
            "Loss:  1.085446834564209\n",
            "Loss:  1.1343165636062622\n",
            "Loss:  1.271946907043457\n",
            "Loss:  1.0911210775375366\n",
            "Loss:  1.0556384325027466\n",
            "Loss:  1.1467281579971313\n",
            "Loss:  1.1846370697021484\n",
            "Loss:  1.2007884979248047\n",
            "Loss:  1.164503812789917\n",
            "Loss:  1.171916127204895\n",
            "Loss:  1.2475067377090454\n",
            "Loss:  1.1632364988327026\n",
            "Loss:  1.1820342540740967\n",
            "Loss:  1.2169241905212402\n",
            "Loss:  1.13082754611969\n",
            "Loss:  1.1444669961929321\n",
            "Loss:  1.2086238861083984\n",
            "Loss:  1.170107364654541\n",
            "Loss:  1.1294993162155151\n",
            "Loss:  1.0493557453155518\n",
            "Loss:  1.1986746788024902\n",
            "Loss:  1.154370903968811\n",
            "Loss:  1.1395502090454102\n",
            "Loss:  1.0483667850494385\n",
            "Loss:  1.1662750244140625\n",
            "Loss:  1.217825174331665\n",
            "Loss:  1.1655080318450928\n",
            "Loss:  1.1577893495559692\n",
            "Loss:  1.1213306188583374\n",
            "Loss:  1.0828015804290771\n",
            "Loss:  1.2413408756256104\n",
            "Loss:  1.0049165487289429\n",
            "Loss:  1.1619311571121216\n",
            "Loss:  1.1800116300582886\n",
            "Loss:  1.1611018180847168\n",
            "Loss:  1.2172293663024902\n",
            "Loss:  1.291195273399353\n",
            "Loss:  1.1159640550613403\n",
            "Loss:  1.2516629695892334\n",
            "Loss:  1.1751240491867065\n",
            "Loss:  1.2071725130081177\n",
            "Loss:  1.1654465198516846\n",
            "Loss:  1.0839165449142456\n",
            "Loss:  1.1327065229415894\n",
            "Loss:  1.212334156036377\n",
            "Loss:  1.1812562942504883\n",
            "Loss:  1.1440608501434326\n",
            "Loss:  1.0924633741378784\n",
            "Loss:  1.3034240007400513\n",
            "Loss:  1.1601239442825317\n",
            "Loss:  1.1940674781799316\n",
            "Loss:  1.2093607187271118\n",
            "Loss:  1.2695612907409668\n",
            "Loss:  1.1038892269134521\n",
            "Loss:  0.9992144107818604\n",
            "Loss:  1.1476500034332275\n",
            "Loss:  1.168988823890686\n",
            "Loss:  1.1397030353546143\n",
            "Loss:  1.1810243129730225\n",
            "Loss:  1.182519793510437\n",
            "Loss:  1.090994119644165\n",
            "Loss:  1.0709140300750732\n",
            "Loss:  1.1263691186904907\n",
            "Loss:  1.1349211931228638\n",
            "Loss:  1.2163922786712646\n",
            "Loss:  1.2331395149230957\n",
            "Loss:  1.0412286520004272\n",
            "Loss:  1.173129677772522\n",
            "Loss:  1.1914197206497192\n",
            "Loss:  1.2332826852798462\n",
            "Loss:  1.1243932247161865\n",
            "Loss:  1.236891269683838\n",
            "Loss:  1.1852467060089111\n",
            "Loss:  1.2283475399017334\n",
            "Loss:  1.2254706621170044\n",
            "Loss:  1.0630015134811401\n",
            "Loss:  1.1495035886764526\n",
            "Loss:  1.1089590787887573\n",
            "Loss:  1.056158185005188\n",
            "Loss:  1.2550723552703857\n",
            "Testing data accuracy: 67%\n",
            "Train Epoch: 5 Loss: 1.024013\n",
            "Train Epoch: 5 Loss: 1.101918\n",
            "Train Epoch: 5 Loss: 1.201277\n",
            "Train Epoch: 5 Loss: 1.096376\n",
            "Train Epoch: 5 Loss: 1.136331\n",
            "Train Epoch: 5 Loss: 1.116008\n",
            "Train Epoch: 5 Loss: 1.088616\n",
            "Train Epoch: 5 Loss: 1.138282\n",
            "Train Epoch: 5 Loss: 1.209178\n",
            "Train Epoch: 5 Loss: 1.139558\n",
            "Train Epoch: 5 Loss: 1.171344\n",
            "Train Epoch: 5 Loss: 1.055206\n",
            "Train Epoch: 5 Loss: 1.253534\n",
            "Train Epoch: 5 Loss: 1.085245\n",
            "Train Epoch: 5 Loss: 1.102375\n",
            "Train Epoch: 5 Loss: 1.270659\n",
            "Train Epoch: 5 Loss: 1.105921\n",
            "Train Epoch: 5 Loss: 1.078455\n",
            "Train Epoch: 5 Loss: 0.955304\n",
            "Train Epoch: 5 Loss: 1.077474\n",
            "Train Epoch: 5 Loss: 1.060396\n",
            "Train Epoch: 5 Loss: 1.124593\n",
            "Train Epoch: 5 Loss: 1.055320\n",
            "Train Epoch: 5 Loss: 1.212723\n",
            "Train Epoch: 5 Loss: 0.923476\n",
            "Train Epoch: 5 Loss: 0.978316\n",
            "Train Epoch: 5 Loss: 1.014580\n",
            "Train Epoch: 5 Loss: 1.081944\n",
            "Train Epoch: 5 Loss: 1.111029\n",
            "Train Epoch: 5 Loss: 0.941996\n",
            "Train Epoch: 5 Loss: 1.160619\n",
            "Train Epoch: 5 Loss: 1.008685\n",
            "Epoch:  5\n",
            "Loss:  1.140191912651062\n",
            "Loss:  0.960030734539032\n",
            "Loss:  0.9777266979217529\n",
            "Loss:  0.9761642217636108\n",
            "Loss:  1.0656160116195679\n",
            "Loss:  1.0645866394042969\n",
            "Loss:  1.0845258235931396\n",
            "Loss:  1.1027348041534424\n",
            "Loss:  1.066951870918274\n",
            "Loss:  1.1965824365615845\n",
            "Loss:  1.0287363529205322\n",
            "Loss:  0.8358773589134216\n",
            "Loss:  1.0579237937927246\n",
            "Loss:  1.0698018074035645\n",
            "Loss:  1.0178959369659424\n",
            "Loss:  1.0283691883087158\n",
            "Loss:  1.035688877105713\n",
            "Loss:  1.151509165763855\n",
            "Loss:  1.0637776851654053\n",
            "Loss:  1.0297001600265503\n",
            "Loss:  1.0124799013137817\n",
            "Loss:  1.0444687604904175\n",
            "Loss:  1.117150902748108\n",
            "Loss:  1.0306295156478882\n",
            "Loss:  1.0849815607070923\n",
            "Loss:  1.0234287977218628\n",
            "Loss:  0.9622230529785156\n",
            "Loss:  0.9465118050575256\n",
            "Loss:  1.0244179964065552\n",
            "Loss:  1.0747497081756592\n",
            "Loss:  1.1735548973083496\n",
            "Loss:  1.026286244392395\n",
            "Loss:  1.0785675048828125\n",
            "Loss:  0.9437100291252136\n",
            "Loss:  0.935287594795227\n",
            "Loss:  1.090868353843689\n",
            "Loss:  0.8999379277229309\n",
            "Loss:  0.9577163457870483\n",
            "Loss:  0.9810410141944885\n",
            "Loss:  0.8675882816314697\n",
            "Loss:  0.9631487727165222\n",
            "Loss:  1.0751301050186157\n",
            "Loss:  1.02928626537323\n",
            "Loss:  1.011759877204895\n",
            "Loss:  1.0230056047439575\n",
            "Loss:  1.0431797504425049\n",
            "Loss:  1.0009138584136963\n",
            "Loss:  0.9768342971801758\n",
            "Loss:  1.1011242866516113\n",
            "Loss:  1.1131072044372559\n",
            "Loss:  1.0537582635879517\n",
            "Loss:  1.1222217082977295\n",
            "Loss:  1.1011245250701904\n",
            "Loss:  1.0791370868682861\n",
            "Loss:  1.0074753761291504\n",
            "Loss:  0.9903609156608582\n",
            "Loss:  0.9690370559692383\n",
            "Loss:  1.0583443641662598\n",
            "Loss:  1.0603461265563965\n",
            "Loss:  1.0389615297317505\n",
            "Loss:  0.9997258186340332\n",
            "Loss:  1.0291837453842163\n",
            "Loss:  1.1057789325714111\n",
            "Loss:  1.1284472942352295\n",
            "Loss:  1.0501846075057983\n",
            "Loss:  1.0198057889938354\n",
            "Loss:  0.8900610208511353\n",
            "Loss:  0.9733861088752747\n",
            "Loss:  0.988397479057312\n",
            "Loss:  1.042356014251709\n",
            "Loss:  1.1749433279037476\n",
            "Loss:  0.9227727055549622\n",
            "Loss:  0.9119313955307007\n",
            "Loss:  1.0750259160995483\n",
            "Loss:  1.0194065570831299\n",
            "Loss:  0.9292272925376892\n",
            "Loss:  1.0559642314910889\n",
            "Loss:  1.144907832145691\n",
            "Loss:  0.9849075078964233\n",
            "Loss:  0.9502924680709839\n",
            "Loss:  1.0587337017059326\n",
            "Loss:  1.0853166580200195\n",
            "Loss:  0.9920150637626648\n",
            "Loss:  1.1005500555038452\n",
            "Loss:  1.0516347885131836\n",
            "Loss:  0.9117717742919922\n",
            "Loss:  1.060197114944458\n",
            "Loss:  1.1069542169570923\n",
            "Loss:  1.0615525245666504\n",
            "Loss:  0.9766121506690979\n",
            "Loss:  1.1668299436569214\n",
            "Loss:  0.9499735236167908\n",
            "Loss:  0.965532124042511\n",
            "Loss:  1.1848379373550415\n",
            "Loss:  0.9921164512634277\n",
            "Loss:  1.0443938970565796\n",
            "Loss:  1.1373881101608276\n",
            "Loss:  1.1210066080093384\n",
            "Loss:  1.0022213459014893\n",
            "Loss:  0.9752230048179626\n",
            "Loss:  1.0541349649429321\n",
            "Loss:  1.0267175436019897\n",
            "Loss:  0.9222403168678284\n",
            "Loss:  0.9412716627120972\n",
            "Loss:  1.1038122177124023\n",
            "Loss:  1.0489856004714966\n",
            "Loss:  0.9898514151573181\n",
            "Loss:  1.065324306488037\n",
            "Loss:  1.1169660091400146\n",
            "Loss:  1.0249816179275513\n",
            "Loss:  0.9414952397346497\n",
            "Loss:  1.0225576162338257\n",
            "Loss:  1.0457308292388916\n",
            "Loss:  1.0011653900146484\n",
            "Loss:  1.1839882135391235\n",
            "Loss:  1.1390546560287476\n",
            "Loss:  1.1630089282989502\n",
            "Loss:  0.9854345917701721\n",
            "Loss:  1.110403060913086\n",
            "Loss:  1.0703539848327637\n",
            "Loss:  0.9328819513320923\n",
            "Loss:  1.1018216609954834\n",
            "Loss:  1.0124348402023315\n",
            "Loss:  0.9838190078735352\n",
            "Loss:  1.0923622846603394\n",
            "Loss:  1.052337408065796\n",
            "Loss:  1.001643180847168\n",
            "Loss:  1.1138194799423218\n",
            "Loss:  0.9363737106323242\n",
            "Loss:  1.0106719732284546\n",
            "Loss:  1.0356329679489136\n",
            "Loss:  1.0525593757629395\n",
            "Loss:  1.0899500846862793\n",
            "Loss:  1.0137547254562378\n",
            "Loss:  1.0211290121078491\n",
            "Loss:  1.1103148460388184\n",
            "Loss:  1.0284032821655273\n",
            "Loss:  0.9379194378852844\n",
            "Loss:  1.022072434425354\n",
            "Loss:  1.1182762384414673\n",
            "Loss:  1.0133506059646606\n",
            "Loss:  1.1602147817611694\n",
            "Loss:  1.0814859867095947\n",
            "Loss:  1.028788447380066\n",
            "Loss:  1.0276247262954712\n",
            "Loss:  1.1292306184768677\n",
            "Loss:  1.022961974143982\n",
            "Loss:  1.0312548875808716\n",
            "Loss:  1.1170716285705566\n",
            "Loss:  1.1107065677642822\n",
            "Loss:  1.008553385734558\n",
            "Loss:  0.9120386242866516\n",
            "Loss:  0.9964127540588379\n",
            "Loss:  0.9387286305427551\n",
            "Loss:  1.0577958822250366\n",
            "Loss:  1.079421877861023\n",
            "Loss:  0.9656305313110352\n",
            "Testing data accuracy: 70%\n",
            "Train Epoch: 6 Loss: 0.951728\n",
            "Train Epoch: 6 Loss: 1.099954\n",
            "Train Epoch: 6 Loss: 1.108905\n",
            "Train Epoch: 6 Loss: 0.982103\n",
            "Train Epoch: 6 Loss: 0.986513\n",
            "Train Epoch: 6 Loss: 0.995232\n",
            "Train Epoch: 6 Loss: 1.026866\n",
            "Train Epoch: 6 Loss: 0.903540\n",
            "Train Epoch: 6 Loss: 1.194375\n",
            "Train Epoch: 6 Loss: 1.134483\n",
            "Train Epoch: 6 Loss: 1.054382\n",
            "Train Epoch: 6 Loss: 0.974707\n",
            "Train Epoch: 6 Loss: 1.016383\n",
            "Train Epoch: 6 Loss: 0.972654\n",
            "Train Epoch: 6 Loss: 0.990037\n",
            "Train Epoch: 6 Loss: 1.047004\n",
            "Train Epoch: 6 Loss: 0.994530\n",
            "Train Epoch: 6 Loss: 0.981105\n",
            "Train Epoch: 6 Loss: 0.933203\n",
            "Train Epoch: 6 Loss: 0.974295\n",
            "Train Epoch: 6 Loss: 0.983013\n",
            "Train Epoch: 6 Loss: 0.986468\n",
            "Train Epoch: 6 Loss: 0.946956\n",
            "Train Epoch: 6 Loss: 0.946106\n",
            "Train Epoch: 6 Loss: 0.921057\n",
            "Train Epoch: 6 Loss: 0.936308\n",
            "Train Epoch: 6 Loss: 0.947529\n",
            "Train Epoch: 6 Loss: 0.943868\n",
            "Train Epoch: 6 Loss: 0.878540\n",
            "Train Epoch: 6 Loss: 1.091920\n",
            "Train Epoch: 6 Loss: 0.941228\n",
            "Train Epoch: 6 Loss: 0.934429\n",
            "Epoch:  6\n",
            "Loss:  0.9881592988967896\n",
            "Loss:  0.949262797832489\n",
            "Loss:  1.1003012657165527\n",
            "Loss:  0.994528591632843\n",
            "Loss:  0.9743638634681702\n",
            "Loss:  1.0188790559768677\n",
            "Loss:  1.0546618700027466\n",
            "Loss:  0.9478046298027039\n",
            "Loss:  1.0567495822906494\n",
            "Loss:  0.9288641214370728\n",
            "Loss:  0.9787010550498962\n",
            "Loss:  0.9458691477775574\n",
            "Loss:  0.879372775554657\n",
            "Loss:  1.077651023864746\n",
            "Loss:  0.9561876654624939\n",
            "Loss:  0.9844290614128113\n",
            "Loss:  1.1079400777816772\n",
            "Loss:  0.9743877053260803\n",
            "Loss:  0.8540323972702026\n",
            "Loss:  0.9119189977645874\n",
            "Loss:  0.9312492609024048\n",
            "Loss:  0.8719366788864136\n",
            "Loss:  0.8220091462135315\n",
            "Loss:  0.7742679715156555\n",
            "Loss:  0.9744526743888855\n",
            "Loss:  0.9283478856086731\n",
            "Loss:  0.9379799365997314\n",
            "Loss:  0.906317412853241\n",
            "Loss:  0.8399308919906616\n",
            "Loss:  0.9589412212371826\n",
            "Loss:  1.0516674518585205\n",
            "Loss:  0.8229789137840271\n",
            "Loss:  0.9284644722938538\n",
            "Loss:  1.0060369968414307\n",
            "Loss:  1.056493878364563\n",
            "Loss:  0.8506797552108765\n",
            "Loss:  0.9132733345031738\n",
            "Loss:  0.8129962682723999\n",
            "Loss:  1.0219449996948242\n",
            "Loss:  1.0387802124023438\n",
            "Loss:  1.0597608089447021\n",
            "Loss:  1.048127293586731\n",
            "Loss:  0.8734768629074097\n",
            "Loss:  0.9091441035270691\n",
            "Loss:  0.8386626839637756\n",
            "Loss:  1.0038341283798218\n",
            "Loss:  0.8777195811271667\n",
            "Loss:  0.7679744958877563\n",
            "Loss:  0.9452755451202393\n",
            "Loss:  1.0334848165512085\n",
            "Loss:  0.9078402519226074\n",
            "Loss:  0.8990762233734131\n",
            "Loss:  1.041883111000061\n",
            "Loss:  0.8338819146156311\n",
            "Loss:  0.9110789895057678\n",
            "Loss:  0.9616281986236572\n",
            "Loss:  0.950359046459198\n",
            "Loss:  0.9111673831939697\n",
            "Loss:  0.8701132535934448\n",
            "Loss:  1.10467529296875\n",
            "Loss:  0.9728414416313171\n",
            "Loss:  0.8186981678009033\n",
            "Loss:  0.9449735879898071\n",
            "Loss:  0.8549281358718872\n",
            "Loss:  0.9539339542388916\n",
            "Loss:  0.9104951620101929\n",
            "Loss:  0.8262765407562256\n",
            "Loss:  0.7855615615844727\n",
            "Loss:  0.9066615104675293\n",
            "Loss:  1.0400527715682983\n",
            "Loss:  1.1116853952407837\n",
            "Loss:  0.9426676034927368\n",
            "Loss:  0.8802472949028015\n",
            "Loss:  1.0996214151382446\n",
            "Loss:  0.7489422559738159\n",
            "Loss:  1.0197900533676147\n",
            "Loss:  0.9260387420654297\n",
            "Loss:  0.9368074536323547\n",
            "Loss:  0.8871543407440186\n",
            "Loss:  0.8931591510772705\n",
            "Loss:  0.8396661877632141\n",
            "Loss:  1.1179229021072388\n",
            "Loss:  0.9432356953620911\n",
            "Loss:  1.05122971534729\n",
            "Loss:  0.923685610294342\n",
            "Loss:  0.901191771030426\n",
            "Loss:  1.0838587284088135\n",
            "Loss:  0.9207955598831177\n",
            "Loss:  1.0148441791534424\n",
            "Loss:  1.0045777559280396\n",
            "Loss:  0.9248552322387695\n",
            "Loss:  0.9440546035766602\n",
            "Loss:  1.0189799070358276\n",
            "Loss:  0.9179078340530396\n",
            "Loss:  0.9466122388839722\n",
            "Loss:  0.8989534974098206\n",
            "Loss:  0.9646251797676086\n",
            "Loss:  0.8717784285545349\n",
            "Loss:  1.0664467811584473\n",
            "Loss:  0.9285608530044556\n",
            "Loss:  0.8100215196609497\n",
            "Loss:  0.9485083818435669\n",
            "Loss:  1.0536363124847412\n",
            "Loss:  0.8146485090255737\n",
            "Loss:  0.8327319622039795\n",
            "Loss:  0.9647718071937561\n",
            "Loss:  0.8567459583282471\n",
            "Loss:  1.1074072122573853\n",
            "Loss:  0.8242647647857666\n",
            "Loss:  1.013352870941162\n",
            "Loss:  0.9742104411125183\n",
            "Loss:  0.892507791519165\n",
            "Loss:  0.9629245400428772\n",
            "Loss:  0.9575666189193726\n",
            "Loss:  1.0404218435287476\n",
            "Loss:  0.9741175770759583\n",
            "Loss:  0.9888662099838257\n",
            "Loss:  1.0455975532531738\n",
            "Loss:  0.8869820237159729\n",
            "Loss:  0.9073564410209656\n",
            "Loss:  0.998496413230896\n",
            "Loss:  0.9061075448989868\n",
            "Loss:  1.0117090940475464\n",
            "Loss:  1.038464069366455\n",
            "Loss:  0.9283716678619385\n",
            "Loss:  0.9628658890724182\n",
            "Loss:  0.9042066335678101\n",
            "Loss:  1.0341548919677734\n",
            "Loss:  0.9982573390007019\n",
            "Loss:  0.8420922160148621\n",
            "Loss:  1.0388808250427246\n",
            "Loss:  0.8570382595062256\n",
            "Loss:  0.919895589351654\n",
            "Loss:  1.0356470346450806\n",
            "Loss:  0.8105219602584839\n",
            "Loss:  1.0419210195541382\n",
            "Loss:  1.1136929988861084\n",
            "Loss:  0.9031878709793091\n",
            "Loss:  1.0831077098846436\n",
            "Loss:  0.8536510467529297\n",
            "Loss:  0.9392741918563843\n",
            "Loss:  1.0003939867019653\n",
            "Loss:  0.8816283345222473\n",
            "Loss:  0.869723379611969\n",
            "Loss:  1.0642523765563965\n",
            "Loss:  0.9189873933792114\n",
            "Loss:  0.8667317628860474\n",
            "Loss:  0.828341007232666\n",
            "Loss:  0.8560391664505005\n",
            "Loss:  0.9864174127578735\n",
            "Loss:  1.0842783451080322\n",
            "Loss:  0.9800293445587158\n",
            "Loss:  0.7444837689399719\n",
            "Loss:  1.0869603157043457\n",
            "Loss:  0.9816843271255493\n",
            "Loss:  1.0181363821029663\n",
            "Loss:  0.8906010389328003\n",
            "Testing data accuracy: 71%\n",
            "Train Epoch: 7 Loss: 0.913045\n",
            "Train Epoch: 7 Loss: 0.882348\n",
            "Train Epoch: 7 Loss: 0.970506\n",
            "Train Epoch: 7 Loss: 0.869909\n",
            "Train Epoch: 7 Loss: 0.963164\n",
            "Train Epoch: 7 Loss: 0.981524\n",
            "Train Epoch: 7 Loss: 0.934274\n",
            "Train Epoch: 7 Loss: 1.073496\n",
            "Train Epoch: 7 Loss: 0.932746\n",
            "Train Epoch: 7 Loss: 0.887403\n",
            "Train Epoch: 7 Loss: 0.890889\n",
            "Train Epoch: 7 Loss: 0.873293\n",
            "Train Epoch: 7 Loss: 0.889098\n",
            "Train Epoch: 7 Loss: 0.920251\n",
            "Train Epoch: 7 Loss: 0.878540\n",
            "Train Epoch: 7 Loss: 0.875222\n",
            "Train Epoch: 7 Loss: 0.845121\n",
            "Train Epoch: 7 Loss: 1.071636\n",
            "Train Epoch: 7 Loss: 0.873487\n",
            "Train Epoch: 7 Loss: 0.894767\n",
            "Train Epoch: 7 Loss: 0.847551\n",
            "Train Epoch: 7 Loss: 0.957495\n",
            "Train Epoch: 7 Loss: 0.833831\n",
            "Train Epoch: 7 Loss: 0.896899\n",
            "Train Epoch: 7 Loss: 0.807099\n",
            "Train Epoch: 7 Loss: 0.829566\n",
            "Train Epoch: 7 Loss: 0.952376\n",
            "Train Epoch: 7 Loss: 0.936051\n",
            "Train Epoch: 7 Loss: 0.898817\n",
            "Train Epoch: 7 Loss: 0.786984\n",
            "Train Epoch: 7 Loss: 0.927577\n",
            "Train Epoch: 7 Loss: 0.938576\n",
            "Epoch:  7\n",
            "Loss:  1.044611930847168\n",
            "Loss:  0.9032900929450989\n",
            "Loss:  0.9336330890655518\n",
            "Loss:  0.8666518926620483\n",
            "Loss:  0.8684753775596619\n",
            "Loss:  0.9862250685691833\n",
            "Loss:  0.7899717688560486\n",
            "Loss:  1.0482633113861084\n",
            "Loss:  0.9440761804580688\n",
            "Loss:  0.8942776918411255\n",
            "Loss:  0.9789014458656311\n",
            "Loss:  0.9451519250869751\n",
            "Loss:  1.008384346961975\n",
            "Loss:  1.0159939527511597\n",
            "Loss:  0.8778005242347717\n",
            "Loss:  0.8063259720802307\n",
            "Loss:  0.7934867739677429\n",
            "Loss:  0.9239215850830078\n",
            "Loss:  0.758435845375061\n",
            "Loss:  0.9484590888023376\n",
            "Loss:  0.9943447113037109\n",
            "Loss:  0.9594812989234924\n",
            "Loss:  0.7748820781707764\n",
            "Loss:  0.9040688872337341\n",
            "Loss:  0.8419317007064819\n",
            "Loss:  0.8354547619819641\n",
            "Loss:  0.7185378670692444\n",
            "Loss:  0.7170003652572632\n",
            "Loss:  0.927412748336792\n",
            "Loss:  0.8494843244552612\n",
            "Loss:  0.9041186571121216\n",
            "Loss:  0.9299259781837463\n",
            "Loss:  1.048927664756775\n",
            "Loss:  0.8174557089805603\n",
            "Loss:  0.8574119806289673\n",
            "Loss:  0.8686370253562927\n",
            "Loss:  0.9482831954956055\n",
            "Loss:  0.8559742569923401\n",
            "Loss:  0.8968704342842102\n",
            "Loss:  0.7913398742675781\n",
            "Loss:  0.8442297577857971\n",
            "Loss:  0.7703325152397156\n",
            "Loss:  0.9154897332191467\n",
            "Loss:  0.7466217279434204\n",
            "Loss:  0.8108137249946594\n",
            "Loss:  0.8651381731033325\n",
            "Loss:  0.8614290952682495\n",
            "Loss:  0.9075130224227905\n",
            "Loss:  1.0432039499282837\n",
            "Loss:  0.8993692398071289\n",
            "Loss:  0.8363649845123291\n",
            "Loss:  0.9302031993865967\n",
            "Loss:  0.7949520945549011\n",
            "Loss:  0.9447062015533447\n",
            "Loss:  0.7086827158927917\n",
            "Loss:  0.9445916414260864\n",
            "Loss:  0.8753138184547424\n",
            "Loss:  0.8119668960571289\n",
            "Loss:  0.8489400744438171\n",
            "Loss:  0.9440768957138062\n",
            "Loss:  0.8013464212417603\n",
            "Loss:  0.8987001180648804\n",
            "Loss:  0.8759180903434753\n",
            "Loss:  0.7922513484954834\n",
            "Loss:  0.8510263562202454\n",
            "Loss:  0.8261987566947937\n",
            "Loss:  0.8498846888542175\n",
            "Loss:  0.9191378355026245\n",
            "Loss:  0.859028160572052\n",
            "Loss:  0.9171835780143738\n",
            "Loss:  0.8602024912834167\n",
            "Loss:  0.7353271842002869\n",
            "Loss:  0.8108871579170227\n",
            "Loss:  0.9566571712493896\n",
            "Loss:  0.7323285341262817\n",
            "Loss:  0.8110447525978088\n",
            "Loss:  1.0705485343933105\n",
            "Loss:  0.9177192449569702\n",
            "Loss:  0.9280236959457397\n",
            "Loss:  1.0326699018478394\n",
            "Loss:  0.9563047885894775\n",
            "Loss:  0.8468501567840576\n",
            "Loss:  0.8325464725494385\n",
            "Loss:  0.8579791188240051\n",
            "Loss:  0.8765750527381897\n",
            "Loss:  1.0633680820465088\n",
            "Loss:  0.8609841465950012\n",
            "Loss:  0.8620947599411011\n",
            "Loss:  0.8189160823822021\n",
            "Loss:  0.8726378679275513\n",
            "Loss:  0.8083966374397278\n",
            "Loss:  1.011278510093689\n",
            "Loss:  0.9811688661575317\n",
            "Loss:  0.9178183674812317\n",
            "Loss:  0.6333591341972351\n",
            "Loss:  0.8949103951454163\n",
            "Loss:  1.0366883277893066\n",
            "Loss:  0.7732393145561218\n",
            "Loss:  0.7908872961997986\n",
            "Loss:  0.8986731171607971\n",
            "Loss:  0.7352046966552734\n",
            "Loss:  1.029395580291748\n",
            "Loss:  0.955118715763092\n",
            "Loss:  0.946709394454956\n",
            "Loss:  0.8462868928909302\n",
            "Loss:  0.9303961396217346\n",
            "Loss:  0.9489072561264038\n",
            "Loss:  0.9055884480476379\n",
            "Loss:  0.9560116529464722\n",
            "Loss:  0.7026770114898682\n",
            "Loss:  0.7944871187210083\n",
            "Loss:  0.8197301626205444\n",
            "Loss:  0.7879048585891724\n",
            "Loss:  0.916265606880188\n",
            "Loss:  0.8566573858261108\n",
            "Loss:  0.9145528674125671\n",
            "Loss:  0.8102688193321228\n",
            "Loss:  0.7905369400978088\n",
            "Loss:  0.9091441035270691\n",
            "Loss:  0.8387170433998108\n",
            "Loss:  0.983463704586029\n",
            "Loss:  0.9085037112236023\n",
            "Loss:  0.8928382992744446\n",
            "Loss:  0.8656664490699768\n",
            "Loss:  0.7917043566703796\n",
            "Loss:  0.8465031981468201\n",
            "Loss:  0.8927977085113525\n",
            "Loss:  1.1547203063964844\n",
            "Loss:  0.9875725507736206\n",
            "Loss:  0.9038609862327576\n",
            "Loss:  0.9041979312896729\n",
            "Loss:  0.833325982093811\n",
            "Loss:  1.0422276258468628\n",
            "Loss:  0.9330630898475647\n",
            "Loss:  0.7427495121955872\n",
            "Loss:  0.8570491075515747\n",
            "Loss:  0.7999677658081055\n",
            "Loss:  0.8466511368751526\n",
            "Loss:  0.8886740803718567\n",
            "Loss:  0.8838567137718201\n",
            "Loss:  0.9020001292228699\n",
            "Loss:  0.9448967576026917\n",
            "Loss:  0.9823858737945557\n",
            "Loss:  0.8291423320770264\n",
            "Loss:  0.9599559307098389\n",
            "Loss:  0.8832719922065735\n",
            "Loss:  0.7786028981208801\n",
            "Loss:  0.9044947624206543\n",
            "Loss:  0.7889009118080139\n",
            "Loss:  0.9795475602149963\n",
            "Loss:  0.7996131181716919\n",
            "Loss:  0.9605510830879211\n",
            "Loss:  0.9547825455665588\n",
            "Loss:  0.9282710552215576\n",
            "Loss:  0.8719930648803711\n",
            "Loss:  0.8633023500442505\n",
            "Loss:  1.1441699266433716\n",
            "Testing data accuracy: 72%\n",
            "Train Epoch: 8 Loss: 0.992860\n",
            "Train Epoch: 8 Loss: 0.801326\n",
            "Train Epoch: 8 Loss: 0.922709\n",
            "Train Epoch: 8 Loss: 0.684158\n",
            "Train Epoch: 8 Loss: 0.923968\n",
            "Train Epoch: 8 Loss: 0.918179\n",
            "Train Epoch: 8 Loss: 0.847793\n",
            "Train Epoch: 8 Loss: 0.892624\n",
            "Train Epoch: 8 Loss: 0.768923\n",
            "Train Epoch: 8 Loss: 0.856576\n",
            "Train Epoch: 8 Loss: 0.770840\n",
            "Train Epoch: 8 Loss: 0.813976\n",
            "Train Epoch: 8 Loss: 0.907681\n",
            "Train Epoch: 8 Loss: 0.779866\n",
            "Train Epoch: 8 Loss: 0.823050\n",
            "Train Epoch: 8 Loss: 0.836689\n",
            "Train Epoch: 8 Loss: 0.849070\n",
            "Train Epoch: 8 Loss: 0.833648\n",
            "Train Epoch: 8 Loss: 0.816323\n",
            "Train Epoch: 8 Loss: 0.807395\n",
            "Train Epoch: 8 Loss: 0.634517\n",
            "Train Epoch: 8 Loss: 0.692789\n",
            "Train Epoch: 8 Loss: 0.924377\n",
            "Train Epoch: 8 Loss: 0.930300\n",
            "Train Epoch: 8 Loss: 0.806376\n",
            "Train Epoch: 8 Loss: 0.907827\n",
            "Train Epoch: 8 Loss: 0.839882\n",
            "Train Epoch: 8 Loss: 0.916685\n",
            "Train Epoch: 8 Loss: 0.911690\n",
            "Train Epoch: 8 Loss: 0.768041\n",
            "Train Epoch: 8 Loss: 0.844329\n",
            "Train Epoch: 8 Loss: 0.774406\n",
            "Epoch:  8\n",
            "Loss:  0.7023128867149353\n",
            "Loss:  0.7447929978370667\n",
            "Loss:  0.9243786334991455\n",
            "Loss:  0.8157278895378113\n",
            "Loss:  0.8380573391914368\n",
            "Loss:  0.8684229850769043\n",
            "Loss:  0.8086456060409546\n",
            "Loss:  0.7569850087165833\n",
            "Loss:  0.9413744211196899\n",
            "Loss:  0.7738620638847351\n",
            "Loss:  0.8117720484733582\n",
            "Loss:  0.870789110660553\n",
            "Loss:  0.6591893434524536\n",
            "Loss:  0.6398574113845825\n",
            "Loss:  0.7905963063240051\n",
            "Loss:  0.8915406465530396\n",
            "Loss:  0.6951590776443481\n",
            "Loss:  0.8093070983886719\n",
            "Loss:  0.745644748210907\n",
            "Loss:  0.7613579034805298\n",
            "Loss:  0.782724142074585\n",
            "Loss:  1.0070562362670898\n",
            "Loss:  0.7925825715065002\n",
            "Loss:  0.888077437877655\n",
            "Loss:  0.8170823454856873\n",
            "Loss:  0.9234165549278259\n",
            "Loss:  0.8296354413032532\n",
            "Loss:  0.9356286525726318\n",
            "Loss:  0.8073555827140808\n",
            "Loss:  0.6890136003494263\n",
            "Loss:  0.7797954082489014\n",
            "Loss:  0.7355956435203552\n",
            "Loss:  1.046294093132019\n",
            "Loss:  0.8441251516342163\n",
            "Loss:  0.8260585069656372\n",
            "Loss:  0.7817469239234924\n",
            "Loss:  0.8454504609107971\n",
            "Loss:  0.7565149068832397\n",
            "Loss:  0.9330933690071106\n",
            "Loss:  0.6922954320907593\n",
            "Loss:  0.7599753737449646\n",
            "Loss:  0.866689920425415\n",
            "Loss:  0.8264732360839844\n",
            "Loss:  0.7560948729515076\n",
            "Loss:  0.8741282224655151\n",
            "Loss:  0.9219046235084534\n",
            "Loss:  0.8681991696357727\n",
            "Loss:  0.8773587346076965\n",
            "Loss:  0.8596137762069702\n",
            "Loss:  1.1039938926696777\n",
            "Loss:  0.8370671272277832\n",
            "Loss:  0.7987293004989624\n",
            "Loss:  0.9784840941429138\n",
            "Loss:  0.752162516117096\n",
            "Loss:  0.8799758553504944\n",
            "Loss:  0.8972460627555847\n",
            "Loss:  0.7824949622154236\n",
            "Loss:  0.7547314763069153\n",
            "Loss:  0.8567276000976562\n",
            "Loss:  0.9628732204437256\n",
            "Loss:  0.6611432433128357\n",
            "Loss:  0.725586473941803\n",
            "Loss:  0.831888735294342\n",
            "Loss:  0.8625916242599487\n",
            "Loss:  0.8237170577049255\n",
            "Loss:  0.7361694574356079\n",
            "Loss:  0.95939040184021\n",
            "Loss:  0.8266352415084839\n",
            "Loss:  0.8816131949424744\n",
            "Loss:  0.7291594743728638\n",
            "Loss:  0.8960428237915039\n",
            "Loss:  0.894330620765686\n",
            "Loss:  0.7151939868927002\n",
            "Loss:  0.6380728483200073\n",
            "Loss:  0.9802528619766235\n",
            "Loss:  0.7900822162628174\n",
            "Loss:  0.7945980429649353\n",
            "Loss:  0.7512688636779785\n",
            "Loss:  0.8955869078636169\n",
            "Loss:  0.9428143501281738\n",
            "Loss:  0.8538662791252136\n",
            "Loss:  0.8344776034355164\n",
            "Loss:  0.9862323999404907\n",
            "Loss:  0.8852271437644958\n",
            "Loss:  0.9313236474990845\n",
            "Loss:  0.7651026844978333\n",
            "Loss:  0.7996163964271545\n",
            "Loss:  0.7848552465438843\n",
            "Loss:  0.7951059341430664\n",
            "Loss:  0.9095641374588013\n",
            "Loss:  0.8986506462097168\n",
            "Loss:  0.8425325155258179\n",
            "Loss:  0.825333833694458\n",
            "Loss:  0.6885530352592468\n",
            "Loss:  0.785586416721344\n",
            "Loss:  0.9181738495826721\n",
            "Loss:  0.7628356218338013\n",
            "Loss:  0.7235752940177917\n",
            "Loss:  0.7924655079841614\n",
            "Loss:  0.9487707018852234\n",
            "Loss:  0.7466793060302734\n",
            "Loss:  0.7043028473854065\n",
            "Loss:  0.8967386484146118\n",
            "Loss:  0.8037149906158447\n",
            "Loss:  0.9177274107933044\n",
            "Loss:  0.9533208012580872\n",
            "Loss:  0.7838743329048157\n",
            "Loss:  0.9407778978347778\n",
            "Loss:  0.8992922902107239\n",
            "Loss:  0.85417640209198\n",
            "Loss:  0.7631455063819885\n",
            "Loss:  0.83721524477005\n",
            "Loss:  0.8088039755821228\n",
            "Loss:  0.8295605778694153\n",
            "Loss:  0.7961714267730713\n",
            "Loss:  0.9800767302513123\n",
            "Loss:  0.6505166888237\n",
            "Loss:  0.8337501287460327\n",
            "Loss:  0.8929564356803894\n",
            "Loss:  0.9305757880210876\n",
            "Loss:  0.6977448463439941\n",
            "Loss:  0.8802531361579895\n",
            "Loss:  0.8846598863601685\n",
            "Loss:  0.833866536617279\n",
            "Loss:  0.9551816582679749\n",
            "Loss:  0.8360662460327148\n",
            "Loss:  0.7647201418876648\n",
            "Loss:  0.7320067882537842\n",
            "Loss:  0.8840190768241882\n",
            "Loss:  0.865425169467926\n",
            "Loss:  0.9357070326805115\n",
            "Loss:  0.771794855594635\n",
            "Loss:  0.6468056440353394\n",
            "Loss:  0.849405825138092\n",
            "Loss:  1.0843571424484253\n",
            "Loss:  0.8634975552558899\n",
            "Loss:  0.8535793423652649\n",
            "Loss:  0.8284239768981934\n",
            "Loss:  0.9112100601196289\n",
            "Loss:  0.888723611831665\n",
            "Loss:  0.7662339806556702\n",
            "Loss:  0.9017326235771179\n",
            "Loss:  0.9221087694168091\n",
            "Loss:  0.8682960271835327\n",
            "Loss:  0.9200305342674255\n",
            "Loss:  0.8323842883110046\n",
            "Loss:  0.9369852542877197\n",
            "Loss:  0.8419647812843323\n",
            "Loss:  0.9094908833503723\n",
            "Loss:  0.7774403095245361\n",
            "Loss:  0.8507663607597351\n",
            "Loss:  0.7529458403587341\n",
            "Loss:  0.9635878801345825\n",
            "Loss:  0.7325125932693481\n",
            "Loss:  0.7942444086074829\n",
            "Loss:  0.8820802569389343\n",
            "Loss:  1.0016776323318481\n",
            "Testing data accuracy: 73%\n",
            "Train Epoch: 9 Loss: 0.890890\n",
            "Train Epoch: 9 Loss: 0.746586\n",
            "Train Epoch: 9 Loss: 0.703575\n",
            "Train Epoch: 9 Loss: 0.771281\n",
            "Train Epoch: 9 Loss: 0.804548\n",
            "Train Epoch: 9 Loss: 0.865898\n",
            "Train Epoch: 9 Loss: 0.959549\n",
            "Train Epoch: 9 Loss: 0.732835\n",
            "Train Epoch: 9 Loss: 0.798025\n",
            "Train Epoch: 9 Loss: 0.738881\n",
            "Train Epoch: 9 Loss: 0.784895\n",
            "Train Epoch: 9 Loss: 0.824300\n",
            "Train Epoch: 9 Loss: 0.660984\n",
            "Train Epoch: 9 Loss: 0.971190\n",
            "Train Epoch: 9 Loss: 0.623376\n",
            "Train Epoch: 9 Loss: 0.852995\n",
            "Train Epoch: 9 Loss: 0.772795\n",
            "Train Epoch: 9 Loss: 0.864552\n",
            "Train Epoch: 9 Loss: 0.818539\n",
            "Train Epoch: 9 Loss: 0.723082\n",
            "Train Epoch: 9 Loss: 0.884626\n",
            "Train Epoch: 9 Loss: 0.827358\n",
            "Train Epoch: 9 Loss: 0.738038\n",
            "Train Epoch: 9 Loss: 0.744462\n",
            "Train Epoch: 9 Loss: 0.962429\n",
            "Train Epoch: 9 Loss: 0.865198\n",
            "Train Epoch: 9 Loss: 0.989127\n",
            "Train Epoch: 9 Loss: 1.022068\n",
            "Train Epoch: 9 Loss: 0.781298\n",
            "Train Epoch: 9 Loss: 0.592422\n",
            "Train Epoch: 9 Loss: 0.717777\n",
            "Train Epoch: 9 Loss: 0.752553\n",
            "Epoch:  9\n",
            "Loss:  0.6851674318313599\n",
            "Loss:  0.9018694162368774\n",
            "Loss:  0.7092061638832092\n",
            "Loss:  0.8426869511604309\n",
            "Loss:  0.8828733563423157\n",
            "Loss:  0.768578052520752\n",
            "Loss:  0.6151299476623535\n",
            "Loss:  0.8143560886383057\n",
            "Loss:  0.7045542597770691\n",
            "Loss:  0.8130112290382385\n",
            "Loss:  0.8659893274307251\n",
            "Loss:  0.8267160654067993\n",
            "Loss:  0.8463813066482544\n",
            "Loss:  0.9006437659263611\n",
            "Loss:  0.8765897750854492\n",
            "Loss:  0.8288960456848145\n",
            "Loss:  0.7415003776550293\n",
            "Loss:  0.7344059944152832\n",
            "Loss:  0.7572674751281738\n",
            "Loss:  0.7068384885787964\n",
            "Loss:  1.023207187652588\n",
            "Loss:  0.7460787296295166\n",
            "Loss:  0.8403559923171997\n",
            "Loss:  0.8941071629524231\n",
            "Loss:  0.8394215106964111\n",
            "Loss:  0.7678917646408081\n",
            "Loss:  0.7388869524002075\n",
            "Loss:  0.8114565014839172\n",
            "Loss:  0.5859224796295166\n",
            "Loss:  0.6851223111152649\n",
            "Loss:  0.8644918203353882\n",
            "Loss:  0.752733051776886\n",
            "Loss:  0.8318964838981628\n",
            "Loss:  0.6773648858070374\n",
            "Loss:  0.8861921429634094\n",
            "Loss:  0.8264034986495972\n",
            "Loss:  0.8449445962905884\n",
            "Loss:  0.9444047212600708\n",
            "Loss:  0.7777023315429688\n",
            "Loss:  0.7584343552589417\n",
            "Loss:  0.7924074530601501\n",
            "Loss:  0.7329744696617126\n",
            "Loss:  0.783785879611969\n",
            "Loss:  0.7342481017112732\n",
            "Loss:  0.8297283053398132\n",
            "Loss:  0.8273531794548035\n",
            "Loss:  0.8161199688911438\n",
            "Loss:  0.7664379477500916\n",
            "Loss:  0.7668466567993164\n",
            "Loss:  0.8669941425323486\n",
            "Loss:  0.6746324896812439\n",
            "Loss:  0.8532967567443848\n",
            "Loss:  0.7092647552490234\n",
            "Loss:  0.8862781524658203\n",
            "Loss:  0.6995490193367004\n",
            "Loss:  0.6821175813674927\n",
            "Loss:  0.7499307990074158\n",
            "Loss:  0.729338526725769\n",
            "Loss:  0.7527430653572083\n",
            "Loss:  0.7795931696891785\n",
            "Loss:  0.8351975083351135\n",
            "Loss:  0.9661295413970947\n",
            "Loss:  0.6950762271881104\n",
            "Loss:  0.7511545419692993\n",
            "Loss:  0.8715618252754211\n",
            "Loss:  0.6666039824485779\n",
            "Loss:  0.7857996821403503\n",
            "Loss:  0.9086700081825256\n",
            "Loss:  0.5368215441703796\n",
            "Loss:  0.8044791221618652\n",
            "Loss:  0.6326599717140198\n",
            "Loss:  0.7589411735534668\n",
            "Loss:  0.6360728740692139\n",
            "Loss:  0.7851241827011108\n",
            "Loss:  0.716450572013855\n",
            "Loss:  0.6565368175506592\n",
            "Loss:  0.7419739961624146\n",
            "Loss:  0.8558708429336548\n",
            "Loss:  0.7087454199790955\n",
            "Loss:  0.7139397859573364\n",
            "Loss:  0.833765983581543\n",
            "Loss:  0.8695030212402344\n",
            "Loss:  0.7536117434501648\n",
            "Loss:  0.8034691214561462\n",
            "Loss:  0.8435158133506775\n",
            "Loss:  0.881741464138031\n",
            "Loss:  0.8651560544967651\n",
            "Loss:  0.7649350166320801\n",
            "Loss:  0.8177477121353149\n",
            "Loss:  0.8533348441123962\n",
            "Loss:  0.8102473616600037\n",
            "Loss:  0.8366907835006714\n",
            "Loss:  0.7321224212646484\n",
            "Loss:  0.945254921913147\n",
            "Loss:  0.8375658988952637\n",
            "Loss:  0.8463956713676453\n",
            "Loss:  0.8365756869316101\n",
            "Loss:  0.8202521204948425\n",
            "Loss:  0.7829319834709167\n",
            "Loss:  0.8422722816467285\n",
            "Loss:  0.7958837151527405\n",
            "Loss:  0.7788796424865723\n",
            "Loss:  0.7525483965873718\n",
            "Loss:  0.7211421132087708\n",
            "Loss:  0.750443696975708\n",
            "Loss:  0.646201491355896\n",
            "Loss:  0.7212236523628235\n",
            "Loss:  0.8566474318504333\n",
            "Loss:  0.9430844187736511\n",
            "Loss:  0.8032440543174744\n",
            "Loss:  0.9058586955070496\n",
            "Loss:  0.7675923109054565\n",
            "Loss:  0.8596604466438293\n",
            "Loss:  0.8720359802246094\n",
            "Loss:  0.8758242726325989\n",
            "Loss:  0.7845864295959473\n",
            "Loss:  0.7674767971038818\n",
            "Loss:  0.7318577766418457\n",
            "Loss:  0.837007999420166\n",
            "Loss:  0.8468639254570007\n",
            "Loss:  0.6689580678939819\n",
            "Loss:  0.7647722363471985\n",
            "Loss:  0.811945378780365\n",
            "Loss:  0.7533337473869324\n",
            "Loss:  0.8094444274902344\n",
            "Loss:  0.7468568682670593\n",
            "Loss:  0.9269083738327026\n",
            "Loss:  0.8057695627212524\n",
            "Loss:  0.9071229696273804\n",
            "Loss:  0.9362084865570068\n",
            "Loss:  0.7650371193885803\n",
            "Loss:  0.7676231265068054\n",
            "Loss:  0.9575822353363037\n",
            "Loss:  0.8093180060386658\n",
            "Loss:  0.997934103012085\n",
            "Loss:  0.634777307510376\n",
            "Loss:  0.8377251029014587\n",
            "Loss:  0.9002702236175537\n",
            "Loss:  0.7980654835700989\n",
            "Loss:  0.880423367023468\n",
            "Loss:  0.739864706993103\n",
            "Loss:  0.9293838739395142\n",
            "Loss:  0.7202962636947632\n",
            "Loss:  0.8396958112716675\n",
            "Loss:  0.8982253670692444\n",
            "Loss:  0.8367131948471069\n",
            "Loss:  0.7824161052703857\n",
            "Loss:  0.8079703450202942\n",
            "Loss:  0.6660877466201782\n",
            "Loss:  0.7784706950187683\n",
            "Loss:  0.8520272374153137\n",
            "Loss:  1.0644457340240479\n",
            "Loss:  0.6913820505142212\n",
            "Loss:  0.7981569766998291\n",
            "Loss:  0.928442120552063\n",
            "Loss:  0.6581392288208008\n",
            "Loss:  0.677952229976654\n",
            "Testing data accuracy: 73%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RITcCNgt1Q55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}